{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d402ad7d",
   "metadata": {},
   "source": [
    "### CNN Coding Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a87b7a",
   "metadata": {},
   "source": [
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a2b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80056b7b",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" alt=\"Alt text\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cf800",
   "metadata": {},
   "source": [
    "2. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c43790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df91d3d",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "```\n",
    "transform = transforms.ToTensor()\n",
    "What ToTensor() does in detail:\n",
    "\n",
    "Before transformation (PIL Image):\n",
    "Format: PIL Image object [PIL means Python Imaging Library]\n",
    "Pixel values: 0-255 (integers)\n",
    "Shape: (28, 28) # Height, Width\n",
    "Data type: uint8\n",
    "\n",
    "After transformation (PyTorch Tensor):\n",
    "Format: torch.Tensor\n",
    "Pixel values: 0.0-1.0 (floats)\n",
    "Shape: (1, 28, 28) # Channels, Height, Width  [2D image with 1 channel]\n",
    "Data type: float32\n",
    "\n",
    "Why this is important:\n",
    "Neural networks work with floating point numbers (0.0-1.0)\n",
    "Adding channel dimension tells network it's grayscale\n",
    "Consistent format for all images\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "Breaking down each parameter:\n",
    "Parameter\t Value\t           What it does\n",
    "root\t    './data'\t       Create folder 'data' in current directory to store dataset\n",
    "train\t     True\t           Load training set (60,000 images) not test set\n",
    "download\t True\t           Download if not already present in ./data\n",
    "transform\ttransform\t       Apply ToTensor() to every image automatically\n",
    "\n",
    "\n",
    "What's inside trainset:\n",
    "trainset is a Dataset object containing:\n",
    "- 60,000 images\n",
    "- 60,000 labels\n",
    "\n",
    "Access like a list:\n",
    "image, label = trainset[0]  # First image and its label\n",
    "image.shape = (1, 28, 28)   # 1 channel (grayscale), 28x28 pixels\n",
    "label = 5                   # Integer 0-9\n",
    "\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "What DataLoader does:\n",
    "\n",
    "\n",
    "With DataLoader (automatic):\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    # images and labels are already batched tensors!\n",
    "    # images.shape = (64, 1, 28, 28)\n",
    "    # labels.shape = (64,)\n",
    "\n",
    "DataLoader parameters explained:\n",
    "Parameter\t      Value\t      What it does\n",
    "batch_size\t       64\t      Group 64 images together\n",
    "shuffle\t          True\t      Randomize order every epoch (prevents learning order bias)\n",
    "\n",
    "How DataLoader organizes data:\n",
    "\n",
    "Before shuffling (example):\n",
    "trainset: [img0, img1, img2, img3, img4, img5, ..., img59999]\n",
    "\n",
    "After shuffling (example):\n",
    "shuffled = [img423, img12876, img5, img38902, img0, ...]\n",
    "\n",
    "Batching with batch_size=64:\n",
    "Batch 0: images[0:64]  → shape (64, 1, 28, 28)\n",
    "Batch 1: images[64:128] → shape (64, 1, 28, 28)\n",
    "...\n",
    "Batch 937: images[59936:60000] → shape (64, 1, 28, 28)\n",
    "Number of batches: 60000/64 = 937.5 → 938 batches (last batch smaller)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac778d",
   "metadata": {},
   "source": [
    "3. Define simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31ebb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):                              # Define a SimpleCNN class that inherits from nn.Module\n",
    "    def __init__(self):                                  # Constructor method to initialize the layers of the SimpleCNN\n",
    "        super(SimpleCNN, self).__init__()                # Call the constructor of the parent class (nn.Module) to properly initialize the SimpleCNN class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)     # Define a convolutional layer with 16 output channels and a kernel size of 3 ; it will learn 16 features from the input image \n",
    "        self.pool = nn.MaxPool2d(2, 2)                   # Define a max pooling layer with a kernel size of 2, reducing the image size by half\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 10)           # Define a linear layer with 16 * 13 * 13 inputs and 10 outputs which means it will learn to classify the input into 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):                                # Define the forward pass of the SimpleCNN, which takes an input tensor x and passes it through the layers defined in self \n",
    "        x = self.pool(torch.relu(self.conv1(x)))         # forward the input through the first convolutional layer and then apply a ReLU activation function\n",
    "        x = x.view(x.size(0), -1)                        # flatten the output of the convolutional layer from a 3D feature map to a 1D vector       \n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46fd85",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "```     \n",
    "self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "nn.Conv2d parameters explained:\n",
    "Parameter\t       Value       \tMeaning\n",
    "in_channels\t        1\t        Input has 1 channel (grayscale image)\n",
    "out_channels       \t16         \tOutput will have 16 channels (16 filters)\n",
    "kernel_size        \t3\t        Filter size is 3×3 pixels\n",
    "\n",
    "What happens inside Conv2d:\n",
    "1. Creates 16 filters (kernels):\n",
    "Filter 1: 3×3 matrix of learnable weights\n",
    "Filter 2: 3×3 matrix of learnable weights\n",
    "...\n",
    "Filter 16: 3×3 matrix of learnable weights\n",
    "Each filter shape: (3, 3)\n",
    "\n",
    "2. Creates 16 biases (one per filter):\n",
    "bias_1, bias_2, ..., bias_16\n",
    "\n",
    "3. Total parameters in conv1:\n",
    "Weights: 16 filters × 3×3 = 16 × 9 = 144 parameters\n",
    "Biases: 16 parameters\n",
    "Total: 160 parameters\n",
    "What convolution does mathematically:\n",
    "For one filter on one position:\n",
    "\n",
    "\n",
    "Input patch (3×3):     Filter (3×3):        Output:\n",
    "[a b c]                [w1 w2 w3]           a×w1 + b×w2 + c×w3\n",
    "[d e f]         ×      [w4 w5 w6]     =     + d×w4 + e×w5 + f×w6\n",
    "[g h i]                [w7 w8 w9]            + g×w7 + h×w8 + i×w9\n",
    "                                           + bias\n",
    "Output size calculation:\n",
    "\n",
    "Input size: 28×28\n",
    "Filter size: 3×3\n",
    "Padding: 0 (default)\n",
    "Stride: 1 (default)  \n",
    "\n",
    "Output size = (28 - 3 + 1) = 26\n",
    "So after conv1: (16, 26, 26) is the output , 16 outputs each with size 26*26 \n",
    "\n",
    "\n",
    "self.pool = nn.MaxPool2d(2, 2)  [reduces size]\n",
    "nn.MaxPool2d parameters:\n",
    "Parameter\t    Value\t      Meaning\n",
    "kernel_size\t      2           Pooling window is 2×2\n",
    "stride\t          2\t          Move window by 2 pixels each step\n",
    "\n",
    "What MaxPool2d does:\n",
    "For each 2×2 region, take the maximum value:\n",
    "\n",
    "Input (4×4):           After MaxPool (2×2):\n",
    "┌───┬───┬───┬───┐     ┌───┬───┐\n",
    "│ 1 │ 3 │ 2 │ 4 │     │ 7 │ 8 │\n",
    "├───┼───┼───┼───┤  →  ├───┼───┤\n",
    "│ 5 │ 7 │ 6 │ 8 │     │15 │16 │\n",
    "├───┼───┼───┼───┤     └───┴───┘\n",
    "│ 9 │11 │10 │12 │\n",
    "├───┼───┼───┼───┤\n",
    "│13 │15 │14 │16 │\n",
    "└───┴───┴───┴───┘\n",
    "\n",
    "Each 2×2 block becomes 1 value (the maximum)\n",
    "Why MaxPool?\n",
    "Reduces size: 26×26 → 13×13 (half the size)\n",
    "Keeps important features: Maximum = strongest activation\n",
    "Adds invariance: Small shifts don't change max much \n",
    "No parameters to learn: Just an operation\n",
    "Output after pool: (16, 13, 13) [output number is same 16, only size is half]\n",
    "\n",
    "\n",
    "\n",
    "self.fc1 = nn.Linear(16 * 13 * 13, 10)  [input vector size, output]\n",
    "Why 16 * 13 * 13?\n",
    "After conv+pool, we have 16 feature maps\n",
    "Each feature map is 13×13 in size\n",
    "Total number of values = 16 × 13 × 13 = 2704 parameters\n",
    "\n",
    "\n",
    "This calculation is CRITICAL:\n",
    "Input image: 28×28\n",
    "After conv1 (no padding): 26×26   [Padding is the technique of adding extra pixels (usually zeros) around the border of an input image before applying convolution.]   \n",
    "\n",
    "After pool (2×2 stride 2): 13×13\n",
    "After 16 filters: 16 × 13 × 13 = 2704\n",
    "\n",
    "nn.Linear parameters:\n",
    "Parameter\t     Value\t      Meaning\n",
    "in_features \t 2704\t      Input vector size\n",
    "out_features\t 10\t          Output size (10 classes for digits 0-9)\n",
    "\n",
    "What Linear layer does:\n",
    "output = input × weight.T + bias\n",
    "\n",
    "input shape: (2704)\n",
    "weight shape: (10, 2704)   [input 2704, 10 output neurons]\n",
    "bias shape: (10)             \n",
    "output shape: (10)         # Logits for each digit\n",
    "\n",
    "Parameters in fc1:\n",
    "Weights: 10 × 2704 = 27,040\n",
    "Biases: 10\n",
    "Total: 27,050 parameters\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "Step-by-step forward pass:\n",
    "Input x shape: (batch_size, 1, 28, 28)     # Example: (64, 1, 28, 28)\n",
    "\n",
    "Line 1: self.conv1(x)\n",
    "\n",
    "Input: (64, 1, 28, 28)\n",
    "Conv2d(1→16, kernel=3)      [from each image we get 16 features or output]\n",
    "Output: (64, 16, 26, 26)\n",
    "Line 1: torch.relu(...)\n",
    "Input: (64, 16, 26, 26)\n",
    "ReLU: max(0, value) for every element\n",
    "Output: (64, 16, 26, 26)  # Same shape, negative values become 0\n",
    " \n",
    "Line 1: self.pool(...)\n",
    "Input: (64, 16, 26, 26)\n",
    "MaxPool2d(2,2): reduces each spatial dimension by half\n",
    "Output: (64, 16, 13, 13)\n",
    "Line 2: x.view(x.size(0), -1)\n",
    "\n",
    "x.size(0) = batch_size = 64\n",
    "-1 means \"infer this dimension\"\n",
    "\n",
    "Before view: (64, 16, 13, 13)\n",
    "After view:  (64, 16×13×13) = (64, 2704)\n",
    "\n",
    "This FLATTENS the 3D feature maps into 1D vectors \n",
    "\n",
    "\n",
    "Line 3: self.fc1(x)\n",
    "Input: (64, 2704)\n",
    "Linear(2704 → 10)\n",
    "Output: (64, 10)    # 64 samples, each with 10 logits [each image gets 10 logits or early prediction]\n",
    "Return: (64, 10) tensor of logits\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff26dd",
   "metadata": {},
   "source": [
    "## 4. Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()                                    # Create an instance of the SimpleCNN class\n",
    "criterion = nn.CrossEntropyLoss()                      # Probabilities -> Avg. Loss -> Gradients -> Weights ,Define the loss function as CrossEntropyLoss, which is commonly used for multi-class classification problems \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)   # loss -> gradients -> weights  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fcf74",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "```\n",
    "model = SimpleCNN()\n",
    "What happens here:\n",
    "Calls __init__() to create all layers\n",
    "Randomly initializes all weights and biases\n",
    "Registers all parameters for optimization\n",
    "Parameter count:\n",
    "conv1 weights: 144  (16 filters, 3×3 matrix of learnable weights)\n",
    "conv1 biases: 16\n",
    "fc1 weights: 27,040 (16 * 13 * 13 * 10)\n",
    "fc1 biases: 10\n",
    "Total: 27,210 parameters\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "What CrossEntropyLoss does:\n",
    "Step 1: Applies Softmax to convert logits to probabilities:\n",
    "logits = [2.0, 1.0, 0.1, -1.0, ..., 3.2]  # 10 values\n",
    "softmax(logits)[i] = exp(logits[i]) / sum(exp(logits[j]))\n",
    "Example:\n",
    "exp(2.0) = 7.39\n",
    "exp(1.0) = 2.72\n",
    "exp(0.1) = 1.11\n",
    "exp(-1.0) = 0.37\n",
    "...\n",
    "sum = 15.6\n",
    "probabilities = [0.47, 0.17, 0.07, 0.02, ...]\n",
    "Step 2: Computes negative log likelihood:\n",
    "If true label = 0:\n",
    "loss = -log(probability[0]) = -log(0.47) = 0.76\n",
    "If true label = 1:\n",
    "loss = -log(probability[1]) = -log(0.17) = 1.77  # Higher! if wrong then punishes more\n",
    "\n",
    "If model is very confident and correct:\n",
    "probability[0] = 0.95 → loss = -log(0.95) = 0.05\n",
    "\n",
    "Step 3: Averages over batch:\n",
    "total_loss = sum(loss for each sample) / batch_size(64)\n",
    "python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "What Adam optimizer does:\n",
    "Adam = Adaptive Moment Estimation\n",
    "\n",
    "Parameters it manages:\n",
    "model.parameters(): All 27,210 learnable weights and biases\n",
    "\n",
    "lr=0.001: Learning rate (step size)\n",
    "What Adam maintains for each parameter:\n",
    "\n",
    "moment_1 = 0  # First moment (mean of gradients)\n",
    "moment_2 = 0  # Second moment (variance of gradients)\n",
    "step = 0      # Number of updates\n",
    "Update rule (simplified):\n",
    "At each step:\n",
    "step += 1\n",
    "# Update moments\n",
    "moment_1 = 0.9 × moment_1 + 0.1 × gradient\n",
    "moment_2 = 0.999 × moment_2 + 0.001 × gradient²\n",
    "\n",
    "# Bias correction\n",
    "m_corrected = moment_1 / (1 - 0.9^step)\n",
    "v_corrected = moment_2 / (1 - 0.999^step)\n",
    "\n",
    "# Update parameter\n",
    "parameter = parameter - lr × m_corrected / (sqrt(v_corrected) + 1e-8)\n",
    "\n",
    "\n",
    "Why Adam:\n",
    "\n",
    "Adapts learning rate per parameter\n",
    "Works well with default settings\n",
    "Converges faster than SGD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f2b88",
   "metadata": {},
   "source": [
    "5. Training loop (very small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 278.862\n",
      "Epoch 2, Loss: 108.298\n",
      "Epoch 3, Loss: 78.554\n",
      "Epoch 4, Loss: 64.227\n",
      "Epoch 5, Loss: 55.757\n",
      "Epoch 6, Loss: 49.155\n",
      "Epoch 7, Loss: 44.339\n",
      "Epoch 8, Loss: 39.983\n",
      "Epoch 9, Loss: 36.913\n",
      "Epoch 10, Loss: 32.992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)                                  # Forward pass: Pass the input images through the model to get the predicted outputs (logits) for each image in the batch\n",
    "        loss = criterion(outputs, labels)                        # Calculate the loss between the model's predictions (outputs) and the true labels (labels) using the defined loss function (criterion)\n",
    "        loss.backward()                                          # Backward pass: Compute the gradients of the loss with respect to the model's parameters (weights) using backpropagation\n",
    "        optimizer.step()                                         # Update the model's parameters using the computed gradients and the defined optimization algorithm (optimizer)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
