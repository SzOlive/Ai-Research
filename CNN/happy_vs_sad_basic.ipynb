{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bf64ec",
   "metadata": {},
   "source": [
    "## Happy vs Sad People CNN classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2190d5",
   "metadata": {},
   "source": [
    "### STEP 1 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50e98ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder                      # A dataset class for loading images from a directory structure where each subdirectory represents a class and contains the images for that class\n",
    "from torch.utils.data import DataLoader, random_split             # A utility for splitting a dataset into random train and validation subsets\n",
    "import os                                                         # A module for interacting with the operating system, used here to list directory contents because we are loading images from a directory structure\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d9ce7",
   "metadata": {},
   "source": [
    "### STEP 2 — Define Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "988c52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:/Users/OLIVE/.cache/kagglehub/datasets/mayank07thakur/happy-vs-sad-people-cnn-classification/versions/1\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/OLIVE/.cache/kagglehub/datasets/mayank07thakur/happy-vs-sad-people-cnn-classification/versions/1\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96148b",
   "metadata": {},
   "source": [
    "### Step 3 — Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f6b0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([            # Define a series of transformations to apply to the images in the dataset; transforms.Compose allows us to chain multiple transformations together\n",
    "    transforms.Resize((128, 128)),          # Resize the images to a fixed size of 128x128 pixels, which is a common preprocessing step for CNNs to ensure that all input images have the same dimensions, 128x128 is a good size for training a CNN on a small dataset like this, as it provides enough detail while keeping the computational requirements manageable\n",
    "    transforms.ToTensor(),                  # Convert the images to PyTorch tensors, which are the primary data structure used in PyTorch for storing and manipulating data; This transformation also scales the pixel values from the range [0, 255] to [0.0, 1.0], which is beneficial for training neural networks\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe621243",
   "metadata": {},
   "source": [
    "### Step 4 — Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b162d1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Happy People', 'Sad People']\n",
      "Total images are: 351\n"
     ]
    }
   ],
   "source": [
    "full_dataset = ImageFolder(root=path, transform=transform)             # Load the dataset using the ImageFolder class, which is a dataset class for loading images from a directory structure where each subdirectory represents a class and contains the images for that class; The root parameter specifies the root directory of the dataset, and the transform parameter specifies the transformations to apply to the images when they are loaded\n",
    "\n",
    "print(\"Classes:\", full_dataset.classes)                                # Print the classes in the dataset, which are determined by the subdirectory names in the root directory; This will show the class labels (e.g., 'Happy People', 'Sad People') that correspond to the images in the dataset\n",
    "print(\"Total images are:\", len(full_dataset))                          # Print the total number of images in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165a40e",
   "metadata": {},
   "source": [
    "### Step 5 — Split Dataset : 80% train, 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c65f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 280\n",
      "Validation size: 71\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(full_dataset))                         #  calculate the size only where 80% of the dataset is used for training\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])       # randomly split the full dataset into a training set and a validation set based on the calculated sizes; random_split takes the full dataset and a list of sizes for each split, and returns two datasets: train_dataset and val_dataset\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Validation size:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b5d03",
   "metadata": {},
   "source": [
    "### Step 6 — Create DataLoaders objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12a0fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)             # Small batch size is fine for small dataset\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f1c6c",
   "metadata": {},
   "source": [
    "### Step 7 — Check Image Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1530c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([16, 3, 128, 128])\n",
      "Labels: tensor([1, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))           # Get a batch of images and labels from the training loader; next(iter(train_loader)) retrieves the first batch of data from the train_loader, which is an iterator that yields batches of images and labels; images will be a tensor containing the image data, and labels will be a tensor containing the corresponding class labels for those images\n",
    "\n",
    "print(\"Image shape:\", images.shape)                 # Print the shape of the images tensor, which will show the batch size, number of channels, and image dimensions (e.g., [16, 3, 128, 128] for a batch of 16 RGB images of size 128x128)\n",
    "print(\"Labels:\", labels[:5])                        # Print the first 5 labels in the batch, which will show the class labels (e.g., 0 for 'Happy People' and 1 for 'Sad People') corresponding to the images in the batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc1e88",
   "metadata": {},
   "source": [
    "### Step 8 — Build Proper CNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2db378ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)                # 3 input channels(RGB), 16 output channels, 3x3 kernel size\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)               # 16 input channels, 32 output channels, 3x3 kernel size\n",
    "        self.pool = nn.MaxPool2d(2,2)                   \n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 30 * 30, 128)         # Fully connected layer with 32*30*30 input features (after two conv and pool layers) and 128 output features\n",
    "        self.fc2 = nn.Linear(128, 2)                    # Output layer with 128 input features and 2 output features (for 2 classes: happy and sad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))       # 128x128 → 126x126 → 63x63\n",
    "        x = self.pool(torch.relu(self.conv2(x)))       # 63x63 → 61x61 → 30x30     \n",
    "        \n",
    "        x = x.view(x.size(0), -1)                      # flatten \n",
    "        \n",
    "        x = torch.relu(self.fc1(x))                    # input : 32x30x30 [32 channels, 30x30 size ], output:128\n",
    "        x = self.fc2(x)                                # output: 2 [binary classification]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28ac4f",
   "metadata": {},
   "source": [
    "### Step 9 — Initialize Model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95987e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56698d7",
   "metadata": {},
   "source": [
    "### Step 10 — Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c3d8834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 13.190, Train Accuracy: 62.86%\n",
      "Epoch 2, Loss: 8.054, Train Accuracy: 76.43%\n",
      "Epoch 3, Loss: 6.155, Train Accuracy: 86.79%\n",
      "Epoch 4, Loss: 4.868, Train Accuracy: 87.50%\n",
      "Epoch 5, Loss: 3.318, Train Accuracy: 94.29%\n",
      "Epoch 6, Loss: 1.828, Train Accuracy: 97.50%\n",
      "Epoch 7, Loss: 1.201, Train Accuracy: 98.21%\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()                           # train() sets the model to training mode, which is necessary for certain layers like dropout and batch normalization to behave correctly during training; In this case, it ensures that the model is in the correct mode for training, even though we don't have those specific layers in our simple CNN\n",
    "    running_loss = 0                        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)                                  # labels.size(0) = batch size , size(0) gives the number of samples in the batch, and we add that to the total count of samples processed\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.3f}, Train Accuracy: {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90814155",
   "metadata": {},
   "source": [
    "### Step 11 — Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d039c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.09859154929578\n"
     ]
    }
   ],
   "source": [
    "model.eval()                              # eval() sets the model to evaluation mode, which is necessary to disable certain layers like dropout and batch normalization that are used during training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(\"Validation Accuracy:\", val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908b4f5",
   "metadata": {},
   "source": [
    "#### Why is validation lower than training?\n",
    "Because model has high capacity and dataset is small, so it memorizes training samples more than validation samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
