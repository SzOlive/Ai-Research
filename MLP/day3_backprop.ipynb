{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9774229",
   "metadata": {},
   "source": [
    "### Day 3 — Backpropagation (Core)\n",
    "\n",
    "**Goals**\n",
    "- Understand why gradients are needed.\n",
    "- Derive gradients for a single neuron (sigmoid) step-by-step.\n",
    "- Implement manual backprop for 1 neuron and autograd for a small MLP in PyTorch.\n",
    "- Be ready to explain `loss.backward()`, `optimizer.zero_grad()` and `optimizer.step()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e648e89",
   "metadata": {},
   "source": [
    "### Youtube video\n",
    "https://youtu.be/Ilg3gGewQ5U?si=EYpJXJrUgL7s8ATf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cb7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9add36",
   "metadata": {},
   "source": [
    "### Quick theory (formulas to memorize + explanation)\n",
    "\n",
    "We consider a **single neuron** with input `x`, weights `w`, bias `b`, and target `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Linear combination (before activation)\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- The neuron first computes a weighted sum of inputs.\n",
    "- Each input is multiplied by its corresponding weight.\n",
    "- Bias `b` shifts the result.\n",
    "\n",
    "Example:\n",
    "If  \n",
    "x = [2, 3]  \n",
    "w = [0.5, 0.8]  \n",
    "b = 1  \n",
    "\n",
    "Then:\n",
    "z = (0.5×2) + (0.8×3) + 1\n",
    "\n",
    "This value `z` is called the **pre-activation** or **logit**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Activation function\n",
    "\n",
    "$$\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- The activation function introduces **non-linearity**.\n",
    "- Sigmoid squashes the output between **0 and 1**.\n",
    "- This is useful for **binary classification**.\n",
    "\n",
    "So:\n",
    "- If z is large → a ≈ 1\n",
    "- If z is small → a ≈ 0\n",
    "\n",
    "`a` is the **predicted output** of the neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Loss function (error measurement)\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}(a - y)^2\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This measures how far the prediction `a` is from the true label `y`.\n",
    "- It is called **Mean Squared Error (MSE)** for one sample.\n",
    "\n",
    "If:\n",
    "- prediction = 0.8\n",
    "- target = 1.0\n",
    "\n",
    "Then:\n",
    "Loss = 0.5 × (0.8 − 1)²\n",
    "\n",
    "The goal of training:\n",
    "→ **Minimize this loss**\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation: Chain rule pieces\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "**How does the loss change if we change the weight?**\n",
    "\n",
    "We compute this using the **chain rule**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Loss with respect to output\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = (a - y)\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This tells us how sensitive the loss is to the output.\n",
    "- If prediction is too big → gradient positive\n",
    "- If prediction is too small → gradient negative\n",
    "\n",
    "This is the **error signal**.This is output.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Output with respect to z\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial z} = a(1-a)\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This is the derivative of the sigmoid function.\n",
    "- It tells us how the output changes when `z` changes.\n",
    "- This term controls how strongly the error flows backward.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: z with respect to weights and bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i\n",
    "\\quad , \\quad\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "From:\n",
    "z = w·x + b\n",
    "\n",
    "If we slightly change a weight:\n",
    "- z changes proportional to the input value.\n",
    "\n",
    "So:\n",
    "- If input is large → weight has big effect\n",
    "- If input is zero → weight has no effect\n",
    "\n",
    "Bias always affects z equally, so:\n",
    "dz/db = 1\n",
    "\n",
    "---\n",
    "\n",
    "## Final combined gradients (chain rule)\n",
    "\n",
    "### Gradient with respect to weight\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "\\cdot\n",
    "x_i\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "Weight update depends on:\n",
    "1. Output error\n",
    "2. Activation sensitivity\n",
    "3. Input value\n",
    "\n",
    "This shows:\n",
    "- If input is zero → weight won’t change\n",
    "- If error is large → weight changes more\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient with respect to bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- Bias update depends only on:\n",
    "  - error\n",
    "  - activation derivative\n",
    "\n",
    "---\n",
    "\n",
    "## Weight update rule (Gradient Descent)\n",
    "\n",
    "After computing gradients:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( \\eta )$ = learning rate \n",
    "- Controls how big the update step is\n",
    "\n",
    "---\n",
    "\n",
    "## Big Picture (Training Steps)\n",
    "\n",
    "1. Forward pass:\n",
    "   - Compute prediction\n",
    "\n",
    "2. Compute loss:\n",
    "   - Compare prediction with target\n",
    "\n",
    "3. Backward pass:\n",
    "   - Compute gradients using chain rule\n",
    "\n",
    "4. Update weights:\n",
    "   - Reduce the loss\n",
    "\n",
    "Repeat many times.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c157d4e",
   "metadata": {},
   "source": [
    "### Code (manual numeric example — single neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88202233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward results:\n",
      " z = 0.0\n",
      " a = 0.5\n",
      " loss = 0.125\n",
      "\n",
      "Gradients (by-hand):\n",
      " dL/da = -0.5\n",
      " da/dz = 0.25\n",
      " dL/dz = -0.125\n",
      " dL/dw0 = -0.125\n",
      " dL/dw1 = -0.0\n",
      " dL/db  = -0.125\n",
      "\n",
      "Weight update (lr=0.1):\n",
      " lr * dL_dw0 = -0.012500000186264515\n",
      " new w0 = 0.012500000186264515\n",
      " new b  = 0.012500000186264515\n"
     ]
    }
   ],
   "source": [
    "# Manual numeric example (single neuron)\n",
    "# Small, reproducible numbers so we can check digit-by-digit.\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 0.0])   # input features\n",
    "y = torch.tensor(1.0)          # target output\n",
    "\n",
    "# Initialize parameters (simple values)\n",
    "w = torch.tensor([0.0, 0.0])   # start at zero for clarity\n",
    "b = torch.tensor(0.0)\n",
    "\n",
    "# Sigmoid function = 1 / (1 + exp(-z))\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + torch.exp(-t))\n",
    "\n",
    "# Forward pass\n",
    "z = w[0]*x[0] + w[1]*x[1] + b  # z = 0*1 + 0*0 + 0 = 0\n",
    "a = sigmoid(z)                  # sigmoid(0) = 0.5\n",
    "\n",
    "# Loss (1/2 (a - y)^2)\n",
    "loss = 0.5 * (a - y)**2\n",
    "\n",
    "print(\"Forward results:\")\n",
    "print(\" z =\", z.item())\n",
    "print(\" a =\", a.item())\n",
    "print(\" loss =\", loss.item())\n",
    "\n",
    "# Backprop by hand (chain rule)\n",
    "dL_da = (a - y)                  # = 0.5 - 1 = -0.5\n",
    "da_dz = a * (1 - a)              # 0.5 * 0.5 = 0.25\n",
    "dL_dz = dL_da * da_dz            # -0.5 * 0.25 = -0.125\n",
    "\n",
    "dL_dw0 = dL_dz * x[0]            # -0.125 * 1 = -0.125 (dl/dw0 = dl/da * da/dz * x[0])\n",
    "dL_dw1 = dL_dz * x[1]            # -0.125 * 0 = -0.0\n",
    "dL_db  = dL_dz * 1.0             # -0.125\n",
    "\n",
    "print(\"\\nGradients (by-hand):\")   # the gradients tell us how to change the weights to reduce the loss\n",
    "print(\" dL/da =\", dL_da.item())\n",
    "print(\" da/dz =\", da_dz.item())\n",
    "print(\" dL/dz =\", dL_dz.item())\n",
    "print(\" dL/dw0 =\", dL_dw0.item())\n",
    "print(\" dL/dw1 =\", dL_dw1.item())\n",
    "print(\" dL/db  =\", dL_db.item())\n",
    "\n",
    "# Weight update example: learning_rate (eta) = 0.1\n",
    "lr = 0.1\n",
    "grad_step = lr * dL_dw0         # -0.0125 \n",
    "new_w0 = w[0] - grad_step       # 0 - (-0.0125) = 0.0125\n",
    "new_b  = b - lr * dL_db         # 0 - 0.1 * (-0.125) = 0.0125\n",
    "\n",
    "print(\"\\nWeight update (lr=0.1):\")\n",
    "print(\" lr * dL_dw0 =\", grad_step.item())\n",
    "print(\" new w0 =\", new_w0.item())\n",
    "print(\" new b  =\", new_b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314f05c",
   "metadata": {},
   "source": [
    "#### **What to observe in the manual loop**\n",
    "- The loss should decrease or move toward lower values.\n",
    "- Signs of gradients determine whether weights increase or decrease.\n",
    "- Every update uses: new_weight = old_weight - lr * gradient.\n",
    "- This manual loop is educational — in practice we use autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0155a9c",
   "metadata": {},
   "source": [
    "### Code (manual backprop training loop — single neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45329fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual training (5 epochs):\n",
      "Epoch 1: loss=0.125000, w=[0.0125 0.    ], b=0.012500\n",
      "Epoch 2: loss=0.121895, w=[0.02484183 0.        ], b=0.024842\n",
      "Epoch 3: loss=0.118868, w=[0.03702385 0.        ], b=0.037024\n",
      "Epoch 4: loss=0.115919, w=[0.04904478 0.        ], b=0.049045\n",
      "Epoch 5: loss=0.113049, w=[0.06090366 0.        ], b=0.060904\n"
     ]
    }
   ],
   "source": [
    "# Manual training loop for the same single-neuron (one-sample) case\n",
    "x = torch.tensor([1.0, 0.0])\n",
    "y = torch.tensor(1.0)\n",
    "\n",
    "# Initialize parameters (float tensors we will update manually)\n",
    "w = torch.tensor([0.0, 0.0])\n",
    "b = torch.tensor(0.0)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + torch.exp(-t))\n",
    "\n",
    "lr = 0.1\n",
    "print(\"Manual training (5 epochs):\")  \n",
    "for epoch in range(1, 6):        # 5 manual updates\n",
    "    # Forward\n",
    "    z = w[0]*x[0] + w[1]*x[1] + b\n",
    "    a = sigmoid(z)\n",
    "    loss = 0.5 * (a - y)**2\n",
    "\n",
    "    # Backprop (same derivatives as above)\n",
    "    dL_da = (a - y)\n",
    "    da_dz = a * (1 - a)\n",
    "    dL_dz = dL_da * da_dz\n",
    "    dL_dw0 = dL_dz * x[0]\n",
    "    dL_dw1 = dL_dz * x[1]\n",
    "    dL_db  = dL_dz * 1.0\n",
    "\n",
    "    # Update (digit-by-digit method)\n",
    "    w = w - lr * torch.tensor([dL_dw0, dL_dw1])\n",
    "    b = b - lr * dL_db\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={loss.item():.6f}, w={w.numpy()}, b={b.item():.6f}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063db48",
   "metadata": {},
   "source": [
    "#### **What to observe in the manual loop**\n",
    "- The loss should decrease or move toward lower values.\n",
    "- Signs of gradients determine whether weights increase or decrease.\n",
    "- Every update uses: new_weight = old_weight - lr * gradient.\n",
    "- This manual loop is educational — in practice we use autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd34a1",
   "metadata": {},
   "source": [
    "### Code (PyTorch autograd example: small MLP single-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7187b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: loss = 0.11452315747737885\n",
      "\n",
      "Gradients (autograd) — mean value per parameter tensor:\n",
      "0.weight: grad mean=-0.006758, shape=torch.Size([3, 2])\n",
      "0.bias: grad mean=-0.013515, shape=torch.Size([3])\n",
      "2.weight: grad mean=-0.083264, shape=torch.Size([1, 3])\n",
      "2.bias: grad mean=-0.151534, shape=torch.Size([1])\n",
      "\n",
      "After 1 optimizer.step(): loss = 0.10778116434812546\n"
     ]
    }
   ],
   "source": [
    "# PyTorch autograd example: small MLP on a single sample to inspect gradients\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Single data point\n",
    "x = torch.tensor([1.0, 0.0])         # shape (2,)\n",
    "y = torch.tensor([1.0])              # shape (1,)\n",
    "\n",
    "# Setup loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Before training: forward and inspect loss\n",
    "pred = model(x)\n",
    "loss = criterion(pred, y)\n",
    "print(\"Before training: loss =\", loss.item())\n",
    "\n",
    "# Backprop with autograd\n",
    "optimizer.zero_grad()\n",
    "loss.backward()   # compute gradients for all parameters\n",
    "\n",
    "print(\"\\nGradients (autograd) — mean value per parameter tensor:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad mean={param.grad.mean().item():.6f}, shape={param.grad.shape}\")\n",
    "\n",
    "# Step (update parameters)\n",
    "optimizer.step()\n",
    "\n",
    "# After one step\n",
    "pred2 = model(x)\n",
    "loss2 = criterion(pred2, y)\n",
    "print(\"\\nAfter 1 optimizer.step(): loss =\", loss2.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
