{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9774229",
   "metadata": {},
   "source": [
    "### Day 3 — Backpropagation (Core)\n",
    "\n",
    "**Goals**\n",
    "- Understand why gradients are needed.\n",
    "- Derive gradients for a single neuron (sigmoid) step-by-step.\n",
    "- Implement manual backprop for 1 neuron and autograd for a small MLP in PyTorch.\n",
    "- Be ready to explain `loss.backward()`, `optimizer.zero_grad()` and `optimizer.step()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e648e89",
   "metadata": {},
   "source": [
    "### Youtube video\n",
    "https://youtu.be/Ilg3gGewQ5U?si=EYpJXJrUgL7s8ATf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cb7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9add36",
   "metadata": {},
   "source": [
    "### Quick theory (formulas to memorize + explanation)\n",
    "\n",
    "We consider a **single neuron** with input `x`, weights `w`, bias `b`, and target `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Linear combination (before activation)\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- The neuron first computes a weighted sum of inputs.\n",
    "- Each input is multiplied by its corresponding weight.\n",
    "- Bias `b` shifts the result.\n",
    "\n",
    "Example:\n",
    "If  \n",
    "x = [2, 3]  \n",
    "w = [0.5, 0.8]  \n",
    "b = 1  \n",
    "\n",
    "Then:\n",
    "z = (0.5×2) + (0.8×3) + 1\n",
    "\n",
    "This value `z` is called the **pre-activation** or **logit**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Activation function\n",
    "\n",
    "$$\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- The activation function introduces **non-linearity**.\n",
    "- Sigmoid squashes the output between **0 and 1**.\n",
    "- This is useful for **binary classification**.\n",
    "\n",
    "So:\n",
    "- If z is large → a ≈ 1\n",
    "- If z is small → a ≈ 0\n",
    "\n",
    "`a` is the **predicted output** of the neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Loss function (error measurement)\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}(a - y)^2\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This measures how far the prediction `a` is from the true label `y`.\n",
    "- It is called **Mean Squared Error (MSE)** for one sample.\n",
    "\n",
    "If:\n",
    "- prediction = 0.8\n",
    "- target = 1.0\n",
    "\n",
    "Then:\n",
    "Loss = 0.5 × (0.8 − 1)²\n",
    "\n",
    "The goal of training:\n",
    "→ **Minimize this loss**\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation: Chain rule pieces\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "**How does the loss change if we change the weight?**\n",
    "\n",
    "We compute this using the **chain rule**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Loss with respect to output\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = (a - y)\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This tells us how sensitive the loss is to the output.\n",
    "- If prediction is too big → gradient positive\n",
    "- If prediction is too small → gradient negative\n",
    "\n",
    "This is the **error signal**.This is output.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Output with respect to z\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial z} = a(1-a)\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- This is the derivative of the sigmoid function.\n",
    "- It tells us how the output changes when `z` changes.\n",
    "- This term controls how strongly the error flows backward.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: z with respect to weights and bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i\n",
    "\\quad , \\quad\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "From:\n",
    "z = w·x + b\n",
    "\n",
    "If we slightly change a weight:\n",
    "- z changes proportional to the input value.\n",
    "\n",
    "So:\n",
    "- If input is large → weight has big effect\n",
    "- If input is zero → weight has no effect\n",
    "\n",
    "Bias always affects z equally, so:\n",
    "dz/db = 1\n",
    "\n",
    "---\n",
    "\n",
    "## Final combined gradients (chain rule)\n",
    "\n",
    "### Gradient with respect to weight\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "\\cdot\n",
    "x_i\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "Weight update depends on:\n",
    "1. Output error\n",
    "2. Activation sensitivity\n",
    "3. Input value\n",
    "\n",
    "This shows:\n",
    "- If input is zero → weight won’t change\n",
    "- If error is large → weight changes more\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient with respect to bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "$$\n",
    "\n",
    "**Meaning:**\n",
    "- Bias update depends only on:\n",
    "  - error\n",
    "  - activation derivative\n",
    "\n",
    "---\n",
    "\n",
    "## Weight update rule (Gradient Descent)\n",
    "\n",
    "After computing gradients:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( \\eta )$ = learning rate \n",
    "- Controls how big the update step is\n",
    "\n",
    "---\n",
    "\n",
    "## Big Picture (Training Steps)\n",
    "\n",
    "1. Forward pass:\n",
    "   - Compute prediction\n",
    "\n",
    "2. Compute loss:\n",
    "   - Compare prediction with target\n",
    "\n",
    "3. Backward pass:\n",
    "   - Compute gradients using chain rule\n",
    "\n",
    "4. Update weights:\n",
    "   - Reduce the loss\n",
    "\n",
    "Repeat many times.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c157d4e",
   "metadata": {},
   "source": [
    "### Code (manual numeric example — single neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88202233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward results:\n",
      " z = 0.0\n",
      " a = 0.5\n",
      " loss = 0.125\n",
      "\n",
      "Gradients (by-hand):\n",
      " dL/da = -0.5\n",
      " da/dz = 0.25\n",
      " dL/dz = -0.125\n",
      " dL/dw0 = -0.125\n",
      " dL/dw1 = -0.0\n",
      " dL/db  = -0.125\n",
      "\n",
      "Weight update (lr=0.1):\n",
      " lr * dL_dw0 = -0.012500000186264515\n",
      " new w0 = 0.012500000186264515\n",
      " new b  = 0.012500000186264515\n"
     ]
    }
   ],
   "source": [
    "# Manual numeric example (single neuron)\n",
    "# Small, reproducible numbers so we can check digit-by-digit.\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 0.0])    # input features\n",
    "y = torch.tensor(1.0)           # target output\n",
    "\n",
    "# Initialize parameters (simple values)\n",
    "w = torch.tensor([0.0, 0.0])    # start at zero for clarity\n",
    "b = torch.tensor(0.0)\n",
    "\n",
    "# Sigmoid function = 1 / (1 + exp(-z))\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + torch.exp(-t))\n",
    "\n",
    "# Forward pass\n",
    "z = w[0]*x[0] + w[1]*x[1] + b  # z = 0*1 + 0*0 + 0 = 0\n",
    "a = sigmoid(z)                  # sigmoid(0) = 0.5\n",
    "\n",
    "# Loss (1/2 (a - y)^2)\n",
    "loss = 0.5 * (a - y)**2\n",
    "\n",
    "print(\"Forward results:\")\n",
    "print(\" z =\", z.item())\n",
    "print(\" a =\", a.item())\n",
    "print(\" loss =\", loss.item())\n",
    "\n",
    "# Backprop by hand (chain rule)\n",
    "dL_da = (a - y)                  # = 0.5 - 1 = -0.5\n",
    "da_dz = a * (1 - a)              # 0.5 * 0.5 = 0.25\n",
    "dL_dz = dL_da * da_dz            # -0.5 * 0.25 = -0.125\n",
    "\n",
    "dL_dw0 = dL_dz * x[0]            # -0.125 * 1 = -0.125 (dl/dw0 = dl/da * da/dz * x[0])\n",
    "dL_dw1 = dL_dz * x[1]            # -0.125 * 0 = -0.0\n",
    "dL_db  = dL_dz * 1.0             # -0.125\n",
    "\n",
    "print(\"\\nGradients (by-hand):\")   # the gradients tell us how to change the weights to reduce the loss\n",
    "print(\" dL/da =\", dL_da.item())\n",
    "print(\" da/dz =\", da_dz.item())\n",
    "print(\" dL/dz =\", dL_dz.item())\n",
    "print(\" dL/dw0 =\", dL_dw0.item())\n",
    "print(\" dL/dw1 =\", dL_dw1.item())\n",
    "print(\" dL/db  =\", dL_db.item())\n",
    "\n",
    "# Weight update example: learning_rate (eta) = 0.1\n",
    "lr = 0.1\n",
    "grad_step = lr * dL_dw0         # -0.0125 \n",
    "new_w0 = w[0] - grad_step       # 0 - (-0.0125) = 0.0125\n",
    "new_b  = b - lr * dL_db         # 0 - 0.1 * (-0.125) = 0.0125\n",
    "\n",
    "print(\"\\nWeight update (lr=0.1):\")\n",
    "print(\" lr * dL_dw0 =\", grad_step.item())\n",
    "print(\" new w0 =\", new_w0.item())\n",
    "print(\" new b  =\", new_b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314f05c",
   "metadata": {},
   "source": [
    "#### **What to observe in the manual loop**\n",
    "- The loss should decrease or move toward lower values.\n",
    "- Signs of gradients determine whether weights increase or decrease.\n",
    "- Every update uses: new_weight = old_weight - lr * gradient.\n",
    "- This manual loop is educational — in practice we use autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0155a9c",
   "metadata": {},
   "source": [
    "### Code (manual backprop training loop — single neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45329fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual training (5 epochs):\n",
      "Epoch 1: loss=0.125000, w=[0.0125 0.    ], b=0.012500\n",
      "Epoch 2: loss=0.121895, w=[0.02484183 0.        ], b=0.024842\n",
      "Epoch 3: loss=0.118868, w=[0.03702385 0.        ], b=0.037024\n",
      "Epoch 4: loss=0.115919, w=[0.04904478 0.        ], b=0.049045\n",
      "Epoch 5: loss=0.113049, w=[0.06090366 0.        ], b=0.060904\n"
     ]
    }
   ],
   "source": [
    "# Manual training loop for the same single-neuron (one-sample) case\n",
    "x = torch.tensor([1.0, 0.0])\n",
    "y = torch.tensor(1.0)\n",
    "\n",
    "# Initialize parameters (float tensors we will update manually)\n",
    "w = torch.tensor([0.0, 0.0])\n",
    "b = torch.tensor(0.0)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + torch.exp(-t))\n",
    "\n",
    "lr = 0.1\n",
    "print(\"Manual training (5 epochs):\")  \n",
    "for epoch in range(1, 6):        # 5 manual updates\n",
    "    # Forward\n",
    "    z = w[0]*x[0] + w[1]*x[1] + b\n",
    "    a = sigmoid(z)\n",
    "    loss = 0.5 * (a - y)**2\n",
    "\n",
    "    # Backprop (same derivatives as above)\n",
    "    dL_da = (a - y)\n",
    "    da_dz = a * (1 - a)\n",
    "    dL_dz = dL_da * da_dz\n",
    "    dL_dw0 = dL_dz * x[0]\n",
    "    dL_dw1 = dL_dz * x[1]\n",
    "    dL_db  = dL_dz * 1.0\n",
    "\n",
    "    # Update (digit-by-digit method)\n",
    "    w = w - lr * torch.tensor([dL_dw0, dL_dw1])\n",
    "    b = b - lr * dL_db\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={loss.item():.6f}, w={w.numpy()}, b={b.item():.6f}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063db48",
   "metadata": {},
   "source": [
    "#### **What to observe in the manual loop**\n",
    "- The loss should decrease or move toward lower values.\n",
    "- Signs of gradients determine whether weights increase or decrease.\n",
    "- Every update uses: new_weight = old_weight - lr * gradient.\n",
    "- This manual loop is educational — in practice we use autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd34a1",
   "metadata": {},
   "source": [
    "### Code (PyTorch autograd example: small MLP single-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7187b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: loss = 0.2756219804286957\n",
      "\n",
      "Gradients (autograd) — mean value per parameter tensor:\n",
      "0.weight: grad mean=0.012418, shape=torch.Size([3, 2])\n",
      "0.bias: grad mean=0.024836, shape=torch.Size([3])\n",
      "2.weight: grad mean=-0.257186, shape=torch.Size([1, 3])\n",
      "2.bias: grad mean=-0.261842, shape=torch.Size([1])\n",
      "\n",
      "After 1 optimizer.step(): loss = 0.2418263852596283\n"
     ]
    }
   ],
   "source": [
    "# PyTorch autograd example: small MLP on a single sample to inspect gradients\n",
    "model = nn.Sequential(      # sequential container of layers,  it stores layers in a list: [layer1, layer2, layer3, layer4]\n",
    "    nn.Linear(2, 3),        # input size 2 (x₁, x₂), output size 3 (3 neurons in hidden layer) with random weights and bias\n",
    "                            # Creates weight matrix W₁: Size 3×2 (3 neurons, 2 weights each)\n",
    "                            # W₁ = [[w₁₁, w₁₂],  Neuron 1 weights for x₁ and x₂\n",
    "                            #      [w₂₁, w₂₂],   Neuron 2 weights  \n",
    "                            #      [w₃₁, w₃₂]]   Neuron 3 weights\n",
    "                            # Creates bias vector b₁: Size 3 (one bias per neuron); b₁ = [b₁, b₂, b₃] with random initial values mostly around zero\n",
    "                            # output = x · W₁ᵀ + b₁\n",
    "    nn.ReLU(),              # output goes through ReLU nonlinearity: max(0, x) applied elementwise\n",
    "\n",
    "    nn.Linear(3, 1),        # 3 inputs (from hidden layer), 1 output with random weights and bias\n",
    "                            # Creates weight matrix W₂: Size 1×3 (1 output neuron, 3 weights for the 3 hidden neurons)\n",
    "                            # W₂ = [[w₁, w₂, w₃]]   Output neuron weights for the 3 hidden neurons\n",
    "                            # Creates bias b₂: Size 1 (one bias for the output neuron); b₂ = [b] with random initial value around zero\n",
    "                            # output = hidden · W₂ᵀ + b₂\n",
    "    nn.Sigmoid()            # final output goes through sigmoid nonlinearity to get a value between 0 and 1\n",
    ")\n",
    "\n",
    "# Single data point\n",
    "x = torch.tensor([1.0, 0.0])         # shape (2,), input features\n",
    "y = torch.tensor([1.0])              # shape (1,), desired output\n",
    "\n",
    "# Setup loss and optimizer functions\n",
    "criterion = nn.MSELoss()                                  # this will compute 1/2 (prediction - y)^2 that is mean squared error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)   # stochastic gradient descent(SGD) optimizer that will update the model parameters based on their gradients\n",
    "                                                          # Update rule: param = param - lr × gradient\n",
    "                                                          # lr=0.1 = learning rate (step size)\n",
    "\n",
    "# Before training: forward pass and inspect loss\n",
    "pred = model(x)                   # forward pass, predicted output\n",
    "loss = criterion(pred, y)         # compute loss\n",
    "print(\"Before training: loss =\", loss.item())\n",
    "\n",
    "# Backprop with autograd, automatically computes gradients for all parameters in the model\n",
    "optimizer.zero_grad()                # this clears old gradients from the last step (otherwise they would accumulate or add up with the new gradients)\n",
    "loss.backward()                      # compute gradients for all parameters, here backward() computes dL/dparam for every parameter in the model and stores it in param.grad\n",
    "\n",
    "print(\"\\nGradients (autograd) — mean value per parameter tensor:\")   # the gradients tell us how to change the weights to reduce the loss\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:                                       # some parameters may not have gradients\n",
    "        print(f\"{name}: grad mean={param.grad.mean().item():.6f}, shape={param.grad.shape}\")      # print mean gradient value and shape of the gradient tensor\n",
    "\n",
    "# Step (update parameters)\n",
    "optimizer.step()            # update parameters based on their gradients and the learning rate\n",
    "                            # equivalent to: param = param - lr * param.grad , weights and biases are updated in-place\n",
    "\n",
    "# After one step\n",
    "pred2 = model(x)            # forward pass after one update with new weights and biases\n",
    "loss2 = criterion(pred2, y) # compute new loss\n",
    "print(\"\\nAfter 1 optimizer.step(): loss =\", loss2.item()) # print loss after one update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977624f",
   "metadata": {},
   "source": [
    "### What happens if learning rate is too large?\n",
    "If the learning rate is too large, the weight updates become too big, causing the training to overshoot the minimum, leading to oscillation or divergence of the loss.\n",
    "\n",
    "lr = 0.01 (good)\n",
    "\n",
    "lr = 1.0 (too large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f9708",
   "metadata": {},
   "source": [
    "### Code (small dataset training loop + loss plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724707b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7105\n",
      "Epoch 20, Loss: 0.6081\n",
      "Epoch 40, Loss: 0.5306\n",
      "Epoch 60, Loss: 0.4522\n",
      "Epoch 80, Loss: 0.3797\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Small random dataset to see backprop in training loop (batch training)\n",
    "from matplotlib.pyplot import text\n",
    "\n",
    "\n",
    "torch.manual_seed(0)       # \n",
    "\n",
    "# Create simple 2D data where label = (x0 + x1 > 0)\n",
    "X = torch.randn(200, 2)     # 200 samples, 2 features each; \n",
    "                            # The actual data (first 5 samples):\n",
    "                                 # X[0] = [1.5410, -0.2934]   # Sample 1\n",
    "                                 # X[1] = [-2.1788, 0.5684]    # Sample 2  \n",
    "                                 # X[2] = [-1.0845, -1.3986]   # Sample 3\n",
    "                                 # X[3] = [0.4033, 0.8380]     # Sample 4\n",
    "                                 # X[4] = [-0.7193, -0.4033]   # Sample 5                              \n",
    "\n",
    "\n",
    "\n",
    "y = (X[:, 0] + X[:, 1] > 0).float().unsqueeze(1)       # actual output, 1 if x0 + x1 > 0, 0 otherwise\n",
    "\n",
    "                                                       # Let's break this into STEPS:\n",
    "\n",
    "                                                       # Step 1: Add the two features                                                   \n",
    "                                                       # sums = X[:, 0] + X[:, 1]\n",
    "                                                       # Takes column 0 + column 1 for each row\n",
    "                                                       \n",
    "                                                       # Result: [1.2476, -1.6104, -2.4831, 1.2413, -1.1226, ...]\n",
    "                                                       \n",
    "                                                       # Step 2: Compare with 0                                                                                                    \n",
    "                                                       # comparison = sums > 0\n",
    "                                                       # Returns boolean (True/False)                                                      \n",
    "                                                       # Result: [True, False, False, True, False, ...]\n",
    "                                                       \n",
    "                                                       # Step 3: Convert to float                                                                                                        \n",
    "                                                       # float_converted = comparison.float()\n",
    "                                                       # True → 1.0, False → 0.0                                                      \n",
    "                                                       # Result: [1.0, 0.0, 0.0, 1.0, 0.0, ...]\n",
    "                                                       \n",
    "                                                       # Step 4: Add dimension with unsqueeze                                                                                                          \n",
    "                                                       # y = float_converted.unsqueeze(1)\n",
    "                                                       # Changes shape from (200,) to (200, 1)                                                       \n",
    "                                                       # Before: [1.0, 0.0, 0.0, ...]                                                       \n",
    "                                                       # After: [[1.0], [0.0], [0.0], ...]\n",
    "                                                       \n",
    "                                                       # Why unsqueeze? BCE loss expects shape (batch, 1), not (batch,)\n",
    "                                                        \n",
    "\n",
    "model2 = nn.Sequential(                    # Input (2) → Linear(2,4) → ReLU → Linear(4,1) → Sigmoid → Output (1)\n",
    "                                           #    x₁ ──────┐\n",
    "                                           #             ├─ [4 neurons] ── [1 neuron] ──► Probability\n",
    "                                           #    x₂ ──────┘\n",
    "                                           # Layer 1: 2×4 = 8 weights + 4 biases = 12 parameters\n",
    "                                           # Layer 2: 4×1 = 4 weights + 1 bias = 5 parameters\n",
    "                                           # Total: 17 learnable parameters\n",
    "    nn.Linear(2, 4),                       # weights and bias are initialized randomly by default\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()                   # binary cross entropy works for this labeling \n",
    "                                           # Formula:\n",
    "                                           # L = -[y × log(p) + (1-y) × log(1-p)]\n",
    "                                           \n",
    "                                           # where:\n",
    "                                           # y = true label (0 or 1)\n",
    "                                           # p = predicted probability\n",
    "\n",
    "                                           # Why BCE for classification?\n",
    "\n",
    "                                           # Punishes confident wrong predictions heavily\n",
    "                                           \n",
    "                                           # If y=1 but p=0.1: loss = -log(0.1) = 2.3\n",
    "                                           \n",
    "                                           # If y=1 but p=0.9: loss = -log(0.9) = 0.1\n",
    "                                           \n",
    "                                           # Difference from MSELoss:\n",
    "                                           \n",
    "                                           # MSE works but gradient vanishes near 0/1\n",
    "                                           \n",
    "                                           # BCE has strong gradients everywhere, especially near 0/1, which helps training\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.1)       # stochastic gradient descent optimizer that will update the model parameters based on their gradients\n",
    "                                                         # Update rule: param = param - lr × gradient\n",
    "                                                         # Example:\n",
    "                                                         # If weight = 0.5, grad = -0.1, lr = 0.1\n",
    "                                                         # New weight = 0.5 - 0.1 × (-0.1) = 0.5 + 0.01 = 0.51 \n",
    "\n",
    "\n",
    "losses = []                                              # Empty list to store loss values for plotting; Purpose: Store loss after each epoch to visualize learning progress\n",
    "for epoch in range(100):                                 # What's an epoch? One complete pass through ALL 200 samples\n",
    "    \n",
    "    pred = model2(X)                                     # Takes ALL 200 samples at once, Output shape: (200, 1) → 200 predictions\n",
    "                                                         # X shape: (200, 2)  # 200 samples, 2 features\n",
    "                                                         # W1 shape: (4, 2)   # 4 neurons, each with 2 weights\n",
    "                                                         # W2 shape: (1, 4)   # 1 neuron, with 4 weights\n",
    "                                                         \n",
    "                                                         # Forward pass using matrix operations:\n",
    "                                                         # h = X @ W1.T + b1    # Shape: (200, 4)\n",
    "                                                         # a = ReLU(h)          # Shape: (200, 4)\n",
    "                                                         # z = a @ W2.T + b2    # Shape: (200, 1)\n",
    "                                                         # pred = Sigmoid(z)    # Shape: (200, 1)\n",
    "                                                      \n",
    "    loss = criterion(pred, y)                            # Compute Loss; What happens:\n",
    "\n",
    "                                                         # Compares 200 predictions with 200 targets\n",
    "                                                         \n",
    "                                                         # Computes BCE for each sample\n",
    "                                                         \n",
    "                                                         # Takes average (default reduction='mean')\n",
    "                                                         \n",
    "                                                         # Numerical example:\n",
    "                                                                                                                  \n",
    "                                                         # pred[0] = 0.8, y[0] = 1.0: loss0 = -log(0.8) = 0.223\n",
    "                                                         # pred[1] = 0.3, y[1] = 0.0: loss1 = -log(0.7) = 0.357\n",
    "                                                         # pred[2] = 0.6, y[2] = 1.0: loss2 = -log(0.6) = 0.511\n",
    "                                                         \n",
    "                                                         # Mean loss = (0.223 + 0.357 + 0.511) / 3 = 0.364\n",
    "                                                         # loss.shape = () (scalar tensor)                  \n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()                               # Clear old gradients from the last step (otherwise they would accumulate or add up with the new gradients)\n",
    "    loss.backward()                                     # This is the heart of training! Traverses computation graph backwards, Applies chain rule at each node, Computes gradients for ALL 17 parameters, stores in param.grad\n",
    "    optimizer.step()                                    # update parameters based on their gradients and the learning rate\n",
    "                                                        # weights and biases are increased or decreased based on the sign of the gradient and the learning rate\n",
    "\n",
    "    losses.append(loss.item())                          # Append the current loss value to the losses list for plotting later; Purpose: Track how the loss changes over epochs to visualize learning progress\n",
    "                                                        # loss.item() converts PyTorch tensor to Python float; Stores in list(losses) for plotting later                \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")     # Print loss every 20 epochs to monitor training progress; Shows how the loss decreases as the model learns\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(6,3))              # Create a new figure with specified size (width=6 inches, height=3 inches) for better visualization\n",
    "plt.plot(losses)                       # plot from the losses list, x-axis is epoch number (0 to 99), y-axis is loss value; Purpose: Visualize how the loss decreases over time as the model learns\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss (small synthetic dataset)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7928e",
   "metadata": {},
   "source": [
    "### What to look for:\n",
    "\n",
    "Good training: Loss decreasing smoothly\n",
    "\n",
    "Overfitting: Loss goes to 0 (not possible here with noise)\n",
    "\n",
    "Underfitting: Loss stays high\n",
    "\n",
    "Divergence: Loss increases (learning rate too high)\n",
    "\n",
    "### WHAT HAPPENS INSIDE THE COMPUTER DURING TRAINING\n",
    "At Epoch 0:\n",
    "\n",
    "Random weights → Random predictions\n",
    "\n",
    "Loss ≈ 0.6931 (because BCE with random 0.5 predictions)\n",
    "\n",
    "Gradients computed\n",
    "\n",
    "Weights updated slightly\n",
    "\n",
    "At Epoch 1:\n",
    "\n",
    "Slightly better weights → Slightly better predictions\n",
    "\n",
    "Loss ≈ 0.6890 (slightly lower)\n",
    "\n",
    "New gradients (smaller magnitude)\n",
    "\n",
    "Weights updated again\n",
    "\n",
    "This continues 100 times...\n",
    "\n",
    "By Epoch 100:\n",
    "\n",
    "Model has learned: \"sum(x₁,x₂) > 0\" pattern\n",
    "\n",
    "Predicts close to 1 for positive sum, close to 0 for negative sum\n",
    "\n",
    "Loss ≈ 0.12\n",
    "\n",
    "\n",
    "### COMMON QUESTIONS ANSWERED\n",
    "Q: Why is loss ~0.69 at start?\n",
    "A: BCE loss for random 0.5 probability = -log(0.5) = 0.6931\n",
    "\n",
    "Q: Why don't we see batches?\n",
    "A: This is full-batch training (all 200 at once). Usually we'd use mini-batches.\n",
    "\n",
    "Q: Why use BCE instead of MSE for classification?\n",
    "A: BCE has stronger gradients near 0/1, faster convergence\n",
    "\n",
    "Q: What if loss doesn't decrease?\n",
    "A: Learning rate too low, model too simple, or data not learnable\n",
    "\n",
    "Q: Why store losses?\n",
    "A: To see if model is actually learning, detect overfitting/divergence\n",
    "\n",
    "### SUMMARY: THE ENTIRE TRAINING PROCESS IN 5 STEPS\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    TRAINING LOOP                        │\n",
    "│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌────────┐   │\n",
    "│  │ Forward  │ →│ Compute  │ →│ Backward │ →│ Update │   │\n",
    "│  │ Pass     │  │ Loss     │  │ Pass     │  │ Weights│   │\n",
    "│  └──────────┘  └──────────┘  └──────────┘  └────────┘   │\n",
    "│         ↑                                        │      │\n",
    "│         └────────────────────────────────────────┘      │\n",
    "│                  Repeat for N epochs                    │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "Data:    200 samples × 2 features\n",
    "Model:   17 parameters\n",
    "Epochs:  100 iterations\n",
    "Updates: 100 weight updates\n",
    "Result:  Model learns \"sum(x₁,x₂) > 0\" pattern\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
