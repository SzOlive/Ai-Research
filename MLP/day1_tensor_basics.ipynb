{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000eb06e",
   "metadata": {},
   "source": [
    "### Day 1: PyTorch Basics and Tensors\n",
    "\n",
    "### 1. Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It is widely used for applications such as computer vision and natural language processing.\n",
    "\n",
    "Why PyTorch?\n",
    "- Dynamic computational graph (which allows for flexible and fast experimentation).\n",
    "- Strong GPU acceleration.\n",
    "- Easy to learn and use, especially for Python users.\n",
    "\n",
    "PyTorch vs. TensorFlow:\n",
    "- TensorFlow uses static computational graphs (though it has dynamic graph capabilities via Eager Execution) while PyTorch uses dynamic computational graphs (which are more intuitive for debugging and building complex models).\n",
    "- PyTorch is more \"Pythonic\" and easier to learn for beginners.\n",
    "\n",
    "### 2. Tensors: The Basic Building Blocks\n",
    "\n",
    "Tensors are a generalized form of vectors and matrices. In PyTorch, tensors are similar to NumPy arrays but with GPU acceleration. Torch is a library where tensor is an array like NumPy.\n",
    "\n",
    "#### 2.1. Creating Tensors\n",
    "\n",
    "Let's create some basic tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa8da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar: tensor(5)\n",
      "Shape of scalar: torch.Size([])\n",
      "\n",
      "Vector: tensor([1, 2, 3])\n",
      "Shape of vector: torch.Size([3])\n",
      "\n",
      "Matrix: tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape of matrix: torch.Size([2, 2])\n",
      "\n",
      "Random Tensor (2x3): tensor([[0.3521, 0.0691, 0.1474],\n",
      "        [0.0759, 0.4095, 0.3250]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar (0-dimensional tensor)\n",
    "scalar = torch.tensor(5)\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Shape of scalar:\", scalar.shape)\n",
    "\n",
    "# Vector (1-dimensional tensor)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"\\nVector:\", vector)\n",
    "print(\"Shape of vector:\", vector.shape)\n",
    "\n",
    "# Matrix (2-dimensional tensor)\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"\\nMatrix:\", matrix)\n",
    "print(\"Shape of matrix:\", matrix.shape)\n",
    "\n",
    "# Random tensor\n",
    "random_tensor = torch.rand(2, 3)  # 2 rows, 3 columns\n",
    "print(\"\\nRandom Tensor (2x3):\", random_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a23d8",
   "metadata": {},
   "source": [
    "### 2.2. Tensor Operations\n",
    "\n",
    "Tensors support various operations. Let's look at a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36984bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([4, 6])\n",
      "a * b = tensor([3, 8])\n",
      "\n",
      "Matrix multiplication result:\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "c = a + b\n",
    "print(\"a + b =\", c)\n",
    "\n",
    "# Multiplication (element-wise)\n",
    "d = a * b\n",
    "print(\"a * b =\", d)\n",
    "\n",
    "# Matrix multiplication\n",
    "e = torch.tensor([[1, 2], [3, 4]])\n",
    "f = torch.tensor([[5, 6], [7, 8]])\n",
    "g = torch.matmul(e, f)\n",
    "print(\"\\nMatrix multiplication result:\\n\", g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680345e8",
   "metadata": {},
   "source": [
    "### 2.3. Tensor Shape and Data Type\n",
    "\n",
    "Understanding the shape and data type of tensors is crucial for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7c8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of random_tensor: torch.Size([2, 3])\n",
      "Data type of random_tensor: torch.float32\n",
      "\n",
      "After converting to int32: torch.int32\n",
      "Integer tensor:\n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape and dtype of the random tensor we created\n",
    "print(\"Shape of random_tensor:\", random_tensor.shape)\n",
    "print(\"Data type of random_tensor:\", random_tensor.dtype)\n",
    "\n",
    "# We can also change the data type (if needed)\n",
    "int_tensor = random_tensor.to(torch.int32)\n",
    "print(\"\\nAfter converting to int32:\", int_tensor.dtype)\n",
    "print(\"Integer tensor:\\n\", int_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d1383",
   "metadata": {},
   "source": [
    "### 2.4. GPU vs CPU\n",
    "\n",
    "PyTorch allows you to move tensors to GPU for faster computations. However, if you don't have a GPU, it will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ecd8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tensor on GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move a tensor to the GPU (if available)\n",
    "if device == 'cuda':\n",
    "    gpu_tensor = random_tensor.to(device)\n",
    "    print(\"Tensor on GPU:\", gpu_tensor.device)\n",
    "else:\n",
    "    print(\"GPU not available, tensor stays on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea030533",
   "metadata": {},
   "source": [
    "### 3. Introduction to MLP (Multi-Layer Perceptron)\n",
    "\n",
    "MLP is a class of feedforward artificial neural network (ANN). It consists of at least three layers:\n",
    "1. Input layer\n",
    "2. One or more hidden layers\n",
    "3. Output layer\n",
    "\n",
    "Each layer (except the input) is fully connected to the next, and each neuron in a layer uses a nonlinear activation function.\n",
    "\n",
    "Why MLP?\n",
    "- Can learn complex patterns.\n",
    "- Universal function approximator (theoretically can approximate any continuous function).\n",
    "\n",
    "### 3.1. Key Components\n",
    "\n",
    "- **Neurons**: Basic units that compute weighted sum of inputs, add bias, and apply activation function.\n",
    "- **Layers**:\n",
    "  - Input layer: Receives the input features.\n",
    "  - Hidden layers: Perform computations and transfer information.\n",
    "  - Output layer: Produces the final output.\n",
    "- **Activation functions**: Introduce non-linearity. Common ones: ReLU, Sigmoid, Tanh.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
