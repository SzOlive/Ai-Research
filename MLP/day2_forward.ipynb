{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1189956",
   "metadata": {},
   "source": [
    "### Day 2: Forward Propagation\n",
    "\n",
    "Goal:\n",
    "Understand how input moves through a neural network to produce an output.\n",
    "\n",
    "Core equation:\n",
    "z = Wx + b\n",
    "\n",
    "a = activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b4d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch           # PyTorch library for tensor computations\n",
    "import torch.nn as nn  # PyTorch's neural network module for defining neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f59e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Neuron Forward Pass\n",
      "Input: tensor([2., 3.])\n",
      "Weights: tensor([0.5000, 0.8000])\n",
      "Bias: tensor(1.)\n",
      "z = w·x + b = 4.400000095367432\n",
      "Output after ReLU: 4.400000095367432\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Neuron Forward Pass\")\n",
    "\n",
    "# Input features\n",
    "x = torch.tensor([2.0, 3.0])    # Input vector of size 2 (2 features)\n",
    "\n",
    "# Weights\n",
    "w = torch.tensor([0.5, 0.8])   # Weights for each input feature\n",
    "\n",
    "# Bias\n",
    "b = torch.tensor(1.0)       \n",
    "\n",
    "# Forward pass\n",
    "z = torch.dot(w, x) + b    # Linear combination (weighted sum + bias), dot product (where each element of w is multiplied by corresponding element of x and then summed up)\n",
    "a = torch.relu(z)          # reLU does not allow negative values, if z<0 then a=0 else a=z\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Weights:\", w)\n",
    "print(\"Bias:\", b)\n",
    "print(\"z = w·x + b =\", z.item())\n",
    "print(\"Output after ReLU:\", a.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26debf2b",
   "metadata": {},
   "source": [
    "Each input has a weight\n",
    "\n",
    "Weighted sum + bias = z\n",
    "\n",
    "Activation function gives final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40ca51",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" alt=\"Alt text\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f52577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Layer Forward Pass\n",
      "Input: tensor([2., 3.])\n",
      "z = W·x + b = tensor([0.9000, 2.0000, 3.1000])\n",
      "Output after ReLU: tensor([0.9000, 2.0000, 3.1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSingle Layer Forward Pass\")\n",
    "\n",
    "# Input (2 features)\n",
    "x = torch.tensor([2.0, 3.0])  # x1,x2\n",
    "\n",
    "# Weight matrix: 3 neurons, 2 inputs  # w11 w12 w21 w22 w31 w32 \n",
    "W = torch.tensor([[0.1, 0.2],\n",
    "                  [0.3, 0.4],\n",
    "                  [0.5, 0.6]])\n",
    "\n",
    "# Bias for each neuron             # b1 b2 b3\n",
    "b = torch.tensor([0.1, 0.2, 0.3])\n",
    "\n",
    "# Forward pass         \n",
    "z = torch.matmul(W, x) + b        # matrix multiplication (W·x) + bias\n",
    "a = torch.relu(z)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"z = W·x + b =\", z)         # z is a vector of size 3 (one for each neuron)\n",
    "print(\"Output after ReLU:\", a)    # a is also a vector of size 3 (one for each neuron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e42be",
   "metadata": {},
   "source": [
    "Each row of W = one neuron\n",
    "\n",
    "We now get multiple outputs\n",
    "\n",
    "This is a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09d006",
   "metadata": {},
   "source": [
    "## Full MLP Forward Pass (2 Layers)\n",
    "\n",
    "Input (2) → Hidden (3) → Output (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646eb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete MLP Forward Pass\n",
      "Hidden layer output: tensor([0.9000, 2.0000, 3.1000])\n",
      "Final output: 0.9955922961235046\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComplete MLP Forward Pass\")\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([2.0, 3.0])\n",
    "\n",
    "# Layer 1: Input → Hidden\n",
    "W1 = torch.tensor([[0.1, 0.2],\n",
    "                   [0.3, 0.4],\n",
    "                   [0.5, 0.6]])\n",
    "b1 = torch.tensor([0.1, 0.2, 0.3])\n",
    "\n",
    "# Layer 2: Hidden → Output\n",
    "W2 = torch.tensor([[0.7, 0.8, 0.9]])\n",
    "b2 = torch.tensor([0.4])\n",
    "\n",
    "# Forward pass\n",
    "z1 = torch.matmul(W1, x) + b1\n",
    "a1 = torch.relu(z1)            # a1 is the output of hidden layer\n",
    "\n",
    "z2 = torch.matmul(W2, a1) + b2\n",
    "output = torch.sigmoid(z2)    # sigmioid function = 1/(1+exp(-z))\n",
    "\n",
    "print(\"Hidden layer output:\", a1)\n",
    "print(\"Final output:\", output.item())     # item() to get scalar value from single-element tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d012828",
   "metadata": {},
   "source": [
    "Multiply input by weights.\n",
    "\n",
    "Add bias.\n",
    "\n",
    "Apply activation.\n",
    "\n",
    "Pass to next layer.\n",
    "\n",
    "Repeat until output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
