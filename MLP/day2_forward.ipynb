{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1189956",
   "metadata": {},
   "source": [
    "### Day 2: Forward Propagation\n",
    "\n",
    "Goal:\n",
    "Understand how input moves through a neural network to produce an output.\n",
    "\n",
    "Core equation:\n",
    "z = Wx + b\n",
    "\n",
    "a = activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b4d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch           # PyTorch library for tensor computations\n",
    "import torch.nn as nn  # PyTorch's neural network module for defining neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f59e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Neuron Forward Pass\n",
      "Input: tensor([2., 3.])\n",
      "Weights: tensor([0.5000, 0.8000])\n",
      "Bias: tensor(1.)\n",
      "z = w·x + b = 4.400000095367432\n",
      "Output after ReLU: 4.400000095367432\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Neuron Forward Pass\")\n",
    "\n",
    "# Input features\n",
    "x = torch.tensor([2.0, 3.0])    # Input vector of size 2 (2 features)\n",
    "\n",
    "# Weights\n",
    "w = torch.tensor([0.5, 0.8])   # Weights for each input feature\n",
    "\n",
    "# Bias\n",
    "b = torch.tensor(1.0)       \n",
    "\n",
    "# Forward pass\n",
    "z = torch.dot(w, x) + b    # Linear combination (weighted sum + bias), dot product (where each element of w is multiplied by corresponding element of x and then summed up)\n",
    "a = torch.relu(z)          # reLU does not allow negative values, if z<0 then a=0 else a=z\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Weights:\", w)\n",
    "print(\"Bias:\", b)\n",
    "print(\"z = w·x + b =\", z.item())\n",
    "print(\"Output after ReLU:\", a.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26debf2b",
   "metadata": {},
   "source": [
    "Each input has a weight\n",
    "\n",
    "Weighted sum + bias = z\n",
    "\n",
    "Activation function gives final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40ca51",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" alt=\"Alt text\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f52577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Layer Forward Pass\n",
      "Input: tensor([2., 3.])\n",
      "z = W·x + b = tensor([0.9000, 2.0000, 3.1000])\n",
      "Output after ReLU: tensor([0.9000, 2.0000, 3.1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSingle Layer Forward Pass\")\n",
    "\n",
    "# Input (2 features)\n",
    "x = torch.tensor([2.0, 3.0])          # x1,x2\n",
    "\n",
    "# Weight matrix: 3 neurons, 2 inputs  # w11 w12 w21 w22 w31 w32 \n",
    "W = torch.tensor([[0.1, 0.2],\n",
    "                  [0.3, 0.4],\n",
    "                  [0.5, 0.6]])\n",
    "\n",
    "# Bias for each neuron                # b1 b2 b3\n",
    "b = torch.tensor([0.1, 0.2, 0.3])\n",
    "\n",
    "# Forward pass         \n",
    "z = torch.matmul(W, x) + b            # matrix multiplication (W·x) + bias\n",
    "a = torch.relu(z)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"z = W·x + b =\", z)             # z is a vector of size 3 (one for each neuron)\n",
    "print(\"Output after ReLU:\", a)        # a is also a vector of size 3 (one for each neuron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e42be",
   "metadata": {},
   "source": [
    "Each row of W = one neuron\n",
    "\n",
    "We now get multiple outputs\n",
    "\n",
    "This is a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09d006",
   "metadata": {},
   "source": [
    "### Full MLP Forward Pass (2 Layers)\n",
    "\n",
    "Input (2) → Hidden (3) → Output (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete MLP Forward Pass\n",
      "Hidden layer output: tensor([0.9000, 2.0000, 3.1000])\n",
      "Final output: 0.9955922961235046\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComplete MLP Forward Pass\")\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([2.0, 3.0])\n",
    "\n",
    "# Layer 1: Input → Hidden\n",
    "W1 = torch.tensor([[0.1, 0.2],\n",
    "                   [0.3, 0.4],\n",
    "                   [0.5, 0.6]])\n",
    "b1 = torch.tensor([0.1, 0.2, 0.3])\n",
    "\n",
    "# Layer 2: Hidden → Output\n",
    "W2 = torch.tensor([[0.7, 0.8, 0.9]])\n",
    "b2 = torch.tensor([0.4])\n",
    "\n",
    "# Forward pass\n",
    "z1 = torch.matmul(W1, x) + b1  # z1 has size 3 (hidden layer output before activation)\n",
    "a1 = torch.relu(z1)            # a1 is the output of hidden layer\n",
    "\n",
    "z2 = torch.matmul(W2, a1) + b2\n",
    "output = torch.sigmoid(z2)    # sigmioid function = 1/(1+exp(-z))\n",
    "\n",
    "print(\"Hidden layer output:\", a1)\n",
    "print(\"Final output:\", output.item())     # item() to get scalar value from single-element tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d012828",
   "metadata": {},
   "source": [
    "Multiply input by weights.\n",
    "\n",
    "Add bias.\n",
    "\n",
    "Apply activation.\n",
    "\n",
    "Pass to next layer.\n",
    "\n",
    "Repeat until output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6df84",
   "metadata": {},
   "source": [
    "### Same Network Using PyTorch Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7dced72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using PyTorch nn.Sequential\n",
      "Model output: 0.6329331398010254\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsing PyTorch nn.Sequential\")\n",
    "\n",
    "model = nn.Sequential( # Sequential container to stack layers together\n",
    "    nn.Linear(2, 3),   # Input → Hidden    # 2 inputs, 3 neurons and randomly initialized weights and biases \n",
    "    nn.ReLU(),                             # 3 neurons 3 outputs from hidden layer \n",
    "    nn.Linear(3, 1),   # Hidden → Output  ;  3 inputs from hidden layer, 1 output neuron and randomly initialized weights and bias\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "x = torch.tensor([2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(\"Model output:\", output.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f92cfb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. Input (2 features) → [Hidden Layer: 3 neurons] → [Output Layer: 1 neuron]\n",
    "\n",
    "       Input Layer        Hidden Layer       Output Layer\n",
    "          [x₁]              [h₁]               [y]    \n",
    "          [x₂]     →        [h₂]     reLU →         -> sigmoid\n",
    "                            [h₃]\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Complete Line-by-Line Breakdown\n",
    "\n",
    "#### Line 1: Creating the Model Structure\n",
    "model = nn.Sequential(\n",
    "What this does:\n",
    "\n",
    "nn.Sequential is a container that stacks layers in sequence\n",
    "\n",
    "Think of it as a pipeline where data flows through each layer in order\n",
    "\n",
    "Like an assembly line: Input → Layer 1 → Layer 2 → ... → Output\n",
    "\n",
    "Analogy: Imagine a factory assembly line:\n",
    "\n",
    "Raw Material → Machine 1 → Machine 2 → Final Product\n",
    "   (Input)    (Layer 1)   (Layer 2)     (Output)\n",
    "\n",
    "#### Line 2: First Layer (Input → Hidden)\n",
    "\n",
    "nn.Linear(2, 3),   # Input → Hidden\n",
    "\n",
    "Breaking down nn.Linear(2, 3):\n",
    "\n",
    "This creates a fully connected layer (also called dense layer)\n",
    "\n",
    "2: Number of input features (must match your data)\n",
    "\n",
    "3: Number of neurons in this layer\n",
    "\n",
    "What happens INSIDE this layer:\n",
    "\n",
    "Weight Matrix Created: Size = 3 × 2 (3 neurons, each with 2 weights)\n",
    "\n",
    "\n",
    "Weights = [[w₁₁, w₁₂],  # Neuron 1: weights for x₁ and x₂\n",
    "           [w₂₁, w₂₂],  # Neuron 2: weights for x₁ and x₂  \n",
    "           [w₃₁, w₃₂]]  # Neuron 3: weights for x₁ and x₂\n",
    "\n",
    "Bias Vector Created: Size = 3 (one bias per neuron)\n",
    "\n",
    "\n",
    "Bias = [b₁, b₂, b₃]\n",
    "\n",
    "Mathematical Operation (for input [x₁, x₂]):\n",
    "\n",
    "\n",
    "For each neuron i:\n",
    "\n",
    "output_i = (w_i₁ × x₁) + (w_i₂ × x₂) + b_i\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "[h₁]   = [w₁₁  w₁₂]   [x₁]   + [b₁]\n",
    "[h₂]     [w₂₁  w₂₂] × [x₂]     [b₂]\n",
    "[h₃]     [w₃₁  w₃₂]            [b₃]\n",
    "\n",
    "Example with numbers (random initialization):\n",
    "\n",
    "```\n",
    " Suppose after initialization:\n",
    " Weights = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
    " Bias = [0.1, 0.2, 0.3]\n",
    " Input x = [2.0, 3.0]\n",
    "```\n",
    "# Calculation:\n",
    "h₁ = (0.1×2) + (0.2×3) + 0.1 = 0.2 + 0.6 + 0.1 = 0.9\n",
    "h₂ = (0.3×2) + (0.4×3) + 0.2 = 0.6 + 1.2 + 0.2 = 2.0\n",
    "h₃ = (0.5×2) + (0.6×3) + 0.3 = 1.0 + 1.8 + 0.3 = 3.1\n",
    "\n",
    "# Output from Linear(2,3): [0.9, 2.0, 3.1]\n",
    "\n",
    "#### Line 3: Activation Function (ReLU)\n",
    "\n",
    "    nn.ReLU(),\n",
    "What ReLU does:\n",
    "\n",
    "ReLU = Rectified Linear Unit\n",
    "\n",
    "Formula: f(x) = max(0, x)\n",
    "\n",
    "Applied element-wise to each neuron's output\n",
    "\n",
    "Continuing our example:\n",
    "\n",
    "\n",
    "Input to ReLU: [0.9, 2.0, 3.1]\n",
    "Apply ReLU: max(0, value) for each value\n",
    "\n",
    "h₁ after ReLU = max(0, 0.9) = 0.9\n",
    "h₂ after ReLU = max(0, 2.0) = 2.0  \n",
    "h₃ after ReLU = max(0, 3.1) = 3.1\n",
    "\n",
    "Output: [0.9, 2.0, 3.1] (all positive, so unchanged)\n",
    "Why ReLU?\n",
    "\n",
    "Introduces non-linearity (without it, network would be linear)\n",
    "\n",
    "Fast computation (just max(0, x))\n",
    "\n",
    "Helps with vanishing gradient problem\n",
    "\n",
    "#### Line 4: Second Layer (Hidden → Output)\n",
    "\n",
    "    nn.Linear(3, 1),   # Hidden → Output\n",
    "Breaking down nn.Linear(3, 1):\n",
    "\n",
    "3: Input features (must match output size of previous layer)\n",
    "\n",
    "1: Number of neurons in output layer\n",
    "\n",
    "What happens INSIDE:\n",
    "\n",
    "Weight Matrix: Size = 1 × 3 (1 neuron with 3 weights)\n",
    "\n",
    "\n",
    "Weights = [[w₁, w₂, w₃]]  # Single neuron, 3 inputs\n",
    "\n",
    "Bias: Single value\n",
    "\n",
    "Bias = [b]\n",
    "Mathematical Operation:\n",
    "\n",
    "y = (w₁ × h₁) + (w₂ × h₂) + (w₃ × h₃) + b\n",
    "Continuing our example (random initialization):\n",
    "\n",
    "# Suppose after initialization:\n",
    "# Weights = [[0.7, 0.8, 0.9]]\n",
    "# Bias = [0.4]\n",
    "# Input from ReLU layer: [0.9, 2.0, 3.1]\n",
    "\n",
    "# Calculation:\n",
    "y = (0.7×0.9) + (0.8×2.0) + (0.9×3.1) + 0.4\n",
    "  = 0.63 + 1.6 + 2.79 + 0.4\n",
    "  = 5.42\n",
    "#### Line 5: Output Activation (Sigmoid)\n",
    "\n",
    "    nn.Sigmoid()\n",
    "What Sigmoid does:\n",
    "\n",
    "Formula: f(x) = 1 / (1 + e⁻ˣ)\n",
    "\n",
    "Squashes output between 0 and 1\n",
    "\n",
    "Continuing our example:\n",
    "\n",
    "Input to Sigmoid: 5.42\n",
    "Apply Sigmoid: 1 / (1 + e⁻⁵·⁴²)\n",
    "\n",
    "Calculate:\n",
    "e⁻⁵·⁴² ≈ 0.0044\n",
    "1 + 0.0044 = 1.0044\n",
    "1 / 1.0044 ≈ 0.9956\n",
    "\n",
    "Final output: 0.9956\n",
    "Why Sigmoid at output?\n",
    "\n",
    "Produces values between 0 and 1\n",
    "\n",
    "Can be interpreted as probability\n",
    "\n",
    "Good for binary classification (yes/no, 1/0)\n",
    "\n",
    "Line 6: Model Definition Complete\n",
    "python\n",
    ")\n",
    "Closes the nn.Sequential container\n",
    "\n",
    "Model is now defined but not trained yet (weights are random)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84e8ae",
   "metadata": {},
   "source": [
    "### Batch Example (Concept Only)\n",
    "\n",
    "Concept:\n",
    "\n",
    "Instead of one input, we process many at once.\n",
    "\n",
    "This is called a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Forward Pass\n",
      "Batch input shape: torch.Size([3, 2])\n",
      "Batch output shape: torch.Size([3, 1])\n",
      "tensor([[0.6329],\n",
      "        [0.6117],\n",
      "        [0.6239]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBatch Forward Pass\")\n",
    "\n",
    "# Batch of 3 samples\n",
    "batch_input = torch.tensor([[2.0, 3.0],        # its shape is (3,2) because we have 3 samples and each sample has 2 features that means 3 input vectors\n",
    "                            [1.0, 4.0],\n",
    "                            [0.5, 0.5]])\n",
    "\n",
    "batch_output = model(batch_input)\n",
    "\n",
    "print(\"Batch input shape:\", batch_input.shape)\n",
    "print(\"Batch output shape:\", batch_output.shape)\n",
    "print(batch_output)                              # output is a vector of size 3 (one for each sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
