{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd28107f",
   "metadata": {},
   "source": [
    "## Full MLP Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2feb",
   "metadata": {},
   "source": [
    "### Step 2.1 — Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b08dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch                                  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b055e7",
   "metadata": {},
   "source": [
    "### Step 2.2 — Load MNIST Dataset\n",
    "\n",
    "What is MNIST?\n",
    "MNIST = Modified National Institute of Standards and Technology\n",
    "\n",
    "70,000 handwritten digits (0-9)\n",
    "\n",
    "Each image: 28×28 pixels (784 total pixels)\n",
    "\n",
    "Grayscale (1 channel) [RGB is 3 channel]\n",
    "\n",
    "What ToTensor() does:\n",
    "Converts PIL Image or numpy array to PyTorch tensor\n",
    "\n",
    "Scales pixel values from [0, 255] to [0.0, 1.0]\n",
    "\n",
    "Changes shape from (H, W) to (C, H, W) → (1, 28, 28) [1 means grayscale]\n",
    "\n",
    "Before transform:\n",
    "```\n",
    "Image: PIL Image object \n",
    "Pixel values: 0-255 (integers) \n",
    "Shape: (28, 28)\n",
    "```\n",
    "After transform:\n",
    "```\n",
    "Tensor: torch.Tensor\n",
    "Pixel values: 0.0-1.0 (floats)\n",
    "Shape: (1, 28, 28)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f11239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:49<00:00, 198kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 42.3kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:09<00:00, 181kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.58MB/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()                   # Convert PIL images to PyTorch tensors and normalize pixel values to [0, 1]\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(         # Load MNIST dataset\n",
    "    root='./data',                                  # Directory to store/download dataset\n",
    "    train=True,                                     # Load training set (60,000 images) for training among the 70,000 total images\n",
    "    transform=transform,                            # Apply ToTensor() to each image \n",
    "    download=True                                   # Download dataset if it doesn't exist\n",
    ")\n",
    "\n",
    "# What this creates:\n",
    "# train_dataset is a PyTorch Dataset object\n",
    "\n",
    "# Contains 60,000 tuples: (image_tensor, label)\n",
    "\n",
    "# image_tensor.shape = (1, 28, 28)     # 1 channel (grayscale), 28x28 pixels; 1 channel because MNIST images are grayscale (not RGB), so we have a single channel; 28x28 is the size of each image\n",
    "\n",
    "# label = integer 0-9\n",
    "\n",
    "# Structure of train_dataset:\n",
    "# train_dataset[0] = (tensor of digit 5, 5)  # 5 is the label, tensor of digit 5 is the image data which is a 1x28x28 tensor; The first element of the tuple is the image tensor, and the second element is the corresponding label (the digit that the image represents)\n",
    "# train_dataset[1] = (tensor of digit 0, 0)\n",
    "# train_dataset[2] = (tensor of digit 4, 4)\n",
    "# ...\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(           # Load MNIST dataset\n",
    "    root='./data',                                   # Directory to store/download dataset\n",
    "    train=False,                                     # Load test set (10,000 images) for evaluation or testing among the 70,000 total images which is separate from the training set and never seen by the model during training\n",
    "    transform=transform                              # Apply ToTensor() to each image\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True)   # Create a DataLoader object to load the training data in batches\n",
    "\n",
    "# What is DataLoader?\n",
    "# A DataLoader wraps a Dataset and provides:\n",
    "# Batching: Groups images into batches\n",
    "# Shuffling: Randomizes order each epoch (important for training)\n",
    "# Parallel loading: Uses multiple CPU cores\n",
    "# Memory efficient: Loads data on-the-fly, not all at once\n",
    "\n",
    "# Parameters explained:\n",
    "# Parameter\t      Value\t            What it does\n",
    "# batch_size\t  64\t            64 images per batch\n",
    "# shuffle\t      True/False\t    Randomize order (True for training, False for testing (for testing shuffling is not necessary))\n",
    "\n",
    "# What happens inside train_loader:\n",
    "# Original dataset: [img0, img1, img2, ..., img59999]\n",
    "# After shuffling: [img423, img12876, img5, img38902, ...]\n",
    "\n",
    "# Batching: \n",
    "# Batch 0: images 423, 12876, 5, 38902, ... (64 images)\n",
    "# Batch 1: next 64 images\n",
    "# ...\n",
    "# Batch 937: last 64 images (since 60000/64 = 937.5 → 938 batches)\n",
    "# Batch 0 shape:\n",
    "# images.shape = (64, 1, 28, 28)   # 64 images, 1 channel, 28x28 pixels\n",
    "# labels.shape = (64,)             # 64 integers labels corresponding to the 64 images in the batch\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False)  # Create a DataLoader object to load the test data in batches; shuffle=False because we don't need to randomize order for evaluation/testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b57d48",
   "metadata": {},
   "source": [
    "\n",
    "### KEY TAKEAWAYS\n",
    "```\n",
    "Why Batch?\n",
    "Speed: Process multiple images at once\n",
    "\n",
    "Stability: Average gradients are less noisy\n",
    "\n",
    "Efficiency: Utilize GPU parallelization\n",
    "\n",
    "Why 64?\n",
    "Power of 2: Optimized for computer hardware\n",
    "\n",
    "Memory sweet spot: Fits well in GPU\n",
    "\n",
    "Good balance: Stable gradients, fast training\n",
    "\n",
    "Why Shuffle?\n",
    "Prevents order bias: Model doesn't learn dataset order\n",
    "\n",
    "Better generalization: Sees mixed examples\n",
    "\n",
    "Stable training: Avoids dramatic weight swings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4909c0",
   "metadata": {},
   "source": [
    "### Step 2.3 — Define the MLP Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90362f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):                            # Define a Multilayer Perceptron (MLP) class that inherits from nn.Module class, which is the base class for all neural network modules in PyTorch\n",
    "    def __init__(self):                          # Constructor method to initialize the layers of the MLP\n",
    "        super(MLP, self).__init__()              # Call the constructor of the parent class (nn.Module) to properly initialize the MLP class\n",
    "        self.model = nn.Sequential(              # Create a sequential container to hold the layers of the MLP\n",
    "            nn.Flatten(),                        # Flatten the input images from (1, 28, 28) to (1, 28*28) = (1, 784) so that it can be fed into the linear layers because linear layers expect 1D input, so now it creates 784 inputs\n",
    "            nn.Linear(28*28, 128),               # Linear layer with 28*28 (784) inputs and 128 outputs which means it will learn 128 features from the input image\n",
    "            nn.ReLU(),                           # ReLU activation function to introduce non-linearity, allowing the model to learn more complex patterns\n",
    "            nn.Linear(128, 64),                  # Linear layer with 128 inputs and 64 outputs which means it will learn 64 features from the previous layer's 128 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)                    # Linear layer with 64 inputs and 10 outputs which means it will learn to classify the input into 10 classes (digits 0-9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):                        # Define the forward pass of the MLP, which takes an input tensor x and passes it through the layers defined in self.model\n",
    "        return self.model(x)                     # Return the output of the model after passing the input through all the layers; This will be a tensor of shape (batch_size, 10) containing the raw scores (logits) for each of the 10 classes for each image in the batch\n",
    "\n",
    "model = MLP()                                    # Create an instance of the MLP class, which initializes the model with the defined architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7921594",
   "metadata": {},
   "source": [
    "```\n",
    "Why create a class?\n",
    "Inherits from nn.Module (gets all PyTorch functionality)\n",
    "\n",
    "__init__: Defines the architecture (layers)\n",
    "\n",
    "forward: Defines how data flows through layers\n",
    "\n",
    "Layer-by-layer breakdown:\n",
    "Layer 1: nn.Flatten()\n",
    "Purpose: Convert 2D image (1,28,28) to 1D vector (784)\n",
    "\n",
    "Before Flatten:\n",
    "\n",
    "\n",
    "Input shape: (64, 1, 28, 28)  # batch, channels, height, width\n",
    "Values: 28×28 grid of pixels\n",
    "After Flatten:\n",
    "\n",
    "\n",
    "Output shape: (64, 784)  # batch, flattened features\n",
    "Values: [pixel0, pixel1, ..., pixel783]            ; these are total 784 inputs from 64 images of a batch\n",
    "Why flatten? MLP expects 1D input, not 2D images.\n",
    "\n",
    "Layer 2: nn.Linear(784, 128)\n",
    "Purpose: First hidden layer with 128 neurons;  784 inputs to 128 neurons\n",
    "\n",
    "What it contains:\n",
    "\n",
    "Weight matrix: 128 × 784 = 100,352 parameters\n",
    "\n",
    "Bias vector: 128 parameters\n",
    "\n",
    "Total: 100,480 parameters in this layer alone!\n",
    "\n",
    "Mathematical operation:\n",
    "\n",
    "\n",
    "h₁ = W₁ × flattened_input + b₁\n",
    "Shape: (64, 128) = (64, 128) × (64, 784)ᵀ? No, careful:\n",
    "\n",
    "Input shape: (64, 784)  \n",
    "W1 shape: (128, 784)\n",
    "Output = Input @ W1.T + b1\n",
    "Shape: (64, 128) = (64, 784) @ (784, 128) + (128,)\n",
    "Layer 3: nn.ReLU()\n",
    "Purpose: Add non-linearity\n",
    "\n",
    "What it does:\n",
    "\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "Before ReLU: [-0.5, 1.2, -3.1, 4.0]\n",
    "After ReLU:  [0.0,  1.2,  0.0, 4.0]\n",
    "Why? Without ReLU, stacking linear layers is still linear!\n",
    "\n",
    "Layer 4: nn.Linear(128, 64)\n",
    "Purpose: Second hidden layer with 64 neurons\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Weight matrix: 64 × 128 = 8,192 parameters\n",
    "\n",
    "Bias vector: 64 parameters\n",
    "\n",
    "Total: 8,256 parameters\n",
    "\n",
    "Layer 5: nn.ReLU()\n",
    "Same as before, adds non-linearity.\n",
    "\n",
    "Layer 6: nn.Linear(64, 10)\n",
    "Purpose: Output layer with 10 neurons (one per digit 0-9)\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Weight matrix: 10 × 64 = 640 parameters\n",
    "\n",
    "Bias vector: 10 parameters\n",
    "\n",
    "Total: 650 parameters\n",
    "\n",
    "Output shape: (64, 10) → 64 samples, each with 10 values (logits)\n",
    "\n",
    "Total parameters:\n",
    "\n",
    "Layer 1: 100,480\n",
    "Layer 2: 8,256\n",
    "Layer 3: 650\n",
    "Total: 109,386 learnable parameters!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1373d",
   "metadata": {},
   "source": [
    "### Step 2.4 — Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()                          # CrossEntropy penalizes wrong confident predictions more; Define the loss function as CrossEntropyLoss, which is commonly used for multi-class classification problems; It combines LogSoftmax and NLLLoss in one single class, making it suitable for our 10-class classification task (digits 0-9), criterion will compute the loss between the predicted logits from the model and the true labels for each batch of data during training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)       # Define the optimizer as Adam, which is an adaptive learning rate optimization algorithm that is widely used for training neural networks; It adjusts the learning rate for each parameter based on the first and second moments of the gradients, allowing for efficient training; lr=0.001 sets the initial learning rate for the optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b2b5f",
   "metadata": {},
   "source": [
    "```\n",
    "What is CrossEntropyLoss?\n",
    "Combines LogSoftmax + NLLLoss (Negative Log Likelihood) in one class.\n",
    "\n",
    "Step-by-step what it does:\n",
    "\n",
    "1. Input to CrossEntropyLoss:\n",
    "\n",
    "Raw model outputs (logits): shape (64, 10)\n",
    "\n",
    "Values can be anything (-∞ to +∞)\n",
    "\n",
    "2. Applies Softmax internally:\n",
    "\n",
    "\n",
    "softmax(x_i) = exp(x_i) / sum(exp(x_i))\n",
    "\n",
    "Example:\n",
    "logits = [2.0, 1.0, 0.1]\n",
    "exp: [7.39, 2.72, 1.11]\n",
    "sum = 11.22\n",
    "softmax: [0.66, 0.24, 0.10]  # Probabilities summing to 1\n",
    "3. Computes negative log likelihood:\n",
    "\n",
    "\n",
    "loss = -log(probability_of_correct_class)\n",
    "\n",
    "\n",
    "Correct class = 0, probability = 0.66\n",
    "loss = -log(0.66) = 0.42\n",
    "\n",
    "If model was confident and wrong:\n",
    "Correct class = 1, probability = 0.24    # wrong \n",
    "loss = -log(0.24) = 1.43  # Higher loss!\n",
    "Why not use MSE?\n",
    "\n",
    "CrossEntropy penalizes wrong confident predictions more\n",
    "\n",
    "Works better with probability outputs\n",
    "\n",
    "Gradient doesn't vanish near 0/1\n",
    "\n",
    "python:\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "What is Adam?\n",
    "Adam = Adaptive Moment Estimation\n",
    "\n",
    "Why Adam instead of SGD?\n",
    "Adaptive learning rates: Different learning rates for different parameters\n",
    "\n",
    "Momentum: Remembers past gradients to smooth updates\n",
    "\n",
    "Works well out-of-the-box: Less hyperparameter tuning\n",
    "\n",
    "What Adam does internally:\n",
    "For each parameter:\n",
    "\n",
    "\n",
    "1. Compute gradient: g_t\n",
    "2. Update biased first moment estimate: m_t = β₁×m_{t-1} + (1-β₁)×g_t\n",
    "3. Update biased second moment estimate: v_t = β₂×v_{t-1} + (1-β₂)×g_t²\n",
    "4. Bias correction\n",
    "5. Update: θ_t = θ_{t-1} - lr × m_t / (√v_t + ε)\n",
    "Parameters:\n",
    "lr=0.001: Learning rate (default for Adam, works well)\n",
    "\n",
    "betas=(0.9, 0.999): Decay rates for moments (default)\n",
    "\n",
    "eps=1e-8: Small number to avoid division by zero\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf8b82",
   "metadata": {},
   "source": [
    "### Step 2.5 — Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c02112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3305\n",
      "Epoch 2, Loss: 0.1371\n",
      "Epoch 3, Loss: 0.0954\n",
      "Epoch 4, Loss: 0.0714\n",
      "Epoch 5, Loss: 0.0570\n"
     ]
    }
   ],
   "source": [
    "epochs = 5                       # total 938 batches of 60000 images will be passed 5 times; Number of times to loop through the entire training dataset during training; More epochs can lead to better learning but also increases training time and risk of overfitting\n",
    "losses = []                      # List to store loss values for each epoch, which can be used for plotting the training loss curve after training is complete\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0           # Initialize running_loss to 0 at the start of each epoch; This variable will be used to accumulate the loss for all batches in the current epoch, allowing us to compute the average loss at the end of the epoch for monitoring training progress \n",
    "                                 # Variable to accumulate the loss for the current epoch; example: if we have 938 batches in each epoch and the loss for each batch is computed, running_loss will sum up all those batch losses, and at the end of the epoch, we will divide running_loss by the number of batches (len(train_loader)) to get the average loss for that epoch\n",
    "    \n",
    "    for images, labels in train_loader:          # Loop over the batches in the training dataset \n",
    "        outputs = model(images)                  # Forward pass:(10 outputs) Pass the input images through the model to get the predicted outputs (logits) for each class; outputs will have shape (batch_size, 10) containing the raw scores for each of the 10 classes for each image in the batch\n",
    "        loss = criterion(outputs, labels)        # Compute the loss by comparing the predicted outputs (logits) with the true labels using the defined loss function (criterion); This will give us a single scalar value representing how well the model's predictions match the true labels for the current batch of data\n",
    "        \n",
    "        optimizer.zero_grad()                    # Zero the gradients of the model's parameters before performing backpropagation; This is important because by default, PyTorch accumulates gradients on subsequent backward passes, so we need to clear the old gradients before computing the new ones for the current batch\n",
    "        loss.backward()                          # Backward pass: Compute the gradients of the loss with respect to the model's parameters using backpropagation; This will populate the .grad attributes of each parameter in the model with the computed gradients based on the current batch's loss\n",
    "        optimizer.step()                         # Update the model's parameters using the computed gradients; This will adjust the weights of the model in the direction that minimizes the loss, based on the optimization algorithm (Adam in this case) and the learning rate\n",
    "        \n",
    "        running_loss += loss.item()              # Accumulate the loss for the current batch into running_loss; loss.item() extracts the scalar value from the loss tensor, allowing us to keep a running total of the loss for all batches in the current epoch\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)    # Compute the average loss for the epoch by dividing the accumulated running_loss by the number of batches (len(train_loader)); This gives us a single scalar value representing the average loss for that epoch, which can be used to monitor training progress and convergence\n",
    "    losses.append(epoch_loss)                        # Append the average loss for the current epoch to the losses list, which can be used later for plotting the training loss curve to visualize how the loss changes over epochs\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")    # Print the epoch number and the average loss for that epoch formatted to 4 decimal places; This provides feedback on the training process, allowing us to see how the loss decreases over time as the model learns from the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da704dc7",
   "metadata": {},
   "source": [
    "#### line by line explained\n",
    "```\n",
    "python\n",
    "epochs = 5\n",
    "losses = []\n",
    "What's an epoch?\n",
    "One complete pass through ALL 60,000 training images.\n",
    "\n",
    "Why 5 epochs?\n",
    "\n",
    "MNIST is simple, learns quickly\n",
    "\n",
    "More epochs might overfit\n",
    "\n",
    "5 is enough to see learning\n",
    "\n",
    "python\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "running_loss accumulates loss for this epoch\n",
    "\n",
    "Reset to 0 at start of each epoch\n",
    "\n",
    "python\n",
    "    for images, labels in train_loader:\n",
    "What happens in one iteration:\n",
    "Data shapes:\n",
    "\n",
    "images.shape: (64, 1, 28, 28)\n",
    "\n",
    "labels.shape: (64,)\n",
    "\n",
    "python\n",
    "        outputs = model(images)\n",
    "Forward pass through all layers:\n",
    "\n",
    "Flatten: (64, 1, 28, 28) → (64, 784)\n",
    "\n",
    "Linear(784→128): (64, 784) → (64, 128)\n",
    "\n",
    "ReLU: (64, 128) → (64, 128) (negative values become 0)\n",
    "\n",
    "Linear(128→64): (64, 128) → (64, 64)\n",
    "\n",
    "ReLU: (64, 64) → (64, 64)\n",
    "\n",
    "Linear(64→10): (64, 64) → (64, 10)\n",
    "\n",
    "Final outputs: Raw scores (logits) for each of 10 digits\n",
    "\n",
    "python\n",
    "        loss = criterion(outputs, labels)\n",
    "What happens inside criterion:\n",
    "\n",
    "\n",
    "loss = 0\n",
    "for i in range(64):  # Each image in batch\n",
    "    # Get probability of correct class using softmax\n",
    "    prob_correct = softmax(outputs[i])[labels[i]]\n",
    "    # Add negative log probability to loss\n",
    "    loss += -log(prob_correct)\n",
    "\n",
    "loss = loss / 64  # Average over batch\n",
    "Example for one image:\n",
    "\n",
    "\n",
    "Outputs = [2.1, 0.5, -1.2, 3.4, 0.1, -0.8, 1.2, -2.1, 0.9, -0.3]\n",
    "Label = 3 (digit 3)\n",
    "\n",
    "Softmax probabilities:\n",
    "[0.08, 0.03, 0.01, 0.30, 0.02, 0.01, 0.03, 0.00, 0.02, 0.00]\n",
    "\n",
    "Probability of class 3 = 0.30\n",
    "Loss = -log(0.30) = 1.20\n",
    "python\n",
    "        optimizer.zero_grad()\n",
    "Why? Gradients accumulate by default. Without zeroing:\n",
    "\n",
    "\n",
    "Batch 1: grad = 0.1\n",
    "Batch 2: grad = 0.1 + 0.2 = 0.3  ← WRONG!\n",
    "Batch 3: grad = 0.1 + 0.2 + 0.3 = 0.6  ← EVEN WORSE!\n",
    "What it does:\n",
    "\n",
    "python\n",
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        param.grad.zero_()\n",
    "python\n",
    "        loss.backward()\n",
    "Computes gradients for ALL 109,386 parameters!\n",
    "\n",
    "What happens internally:\n",
    "\n",
    "Starts at loss (scalar)\n",
    "\n",
    "Traverses computation graph backwards\n",
    "\n",
    "Applies chain rule at each node\n",
    "\n",
    "Stores gradients in param.grad\n",
    "\n",
    "Example gradient path:\n",
    "\n",
    "\n",
    "loss ← CrossEntropy ← Linear(64→10) ← ReLU ← Linear(128→64) ← ReLU ← Linear(784→128) ← Flatten ← images\n",
    "   ↑              ↑         ↑            ↑         ↑            ↑         ↑                ↑\n",
    "   ∂L/∂logits    ∂logits/∂h3  ∂h3/∂a2   ∂a2/∂h2   ∂h2/∂a1    ∂a1/∂h1   ∂h1/∂flattened  ∂flattened/∂images\n",
    "python\n",
    "        optimizer.step()\n",
    "Updates all parameters using gradients:\n",
    "\n",
    "python\n",
    "for param in model.parameters():\n",
    "    # Adam update rule (simplified)\n",
    "    param.data = param.data - learning_rate * adjusted_gradient\n",
    "Example for one weight:\n",
    "\n",
    "\n",
    "Before: weight = 0.5\n",
    "Gradient: -0.02\n",
    "Learning rate: 0.001\n",
    "\n",
    "After: weight = 0.5 - 0.001 × (-0.02) = 0.5 + 0.00002 = 0.50002\n",
    "python\n",
    "        running_loss += loss.item()\n",
    "loss.item() converts PyTorch tensor to Python float\n",
    "\n",
    "Adds to running total for this epoch\n",
    "\n",
    "python\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "len(train_loader) = number of batches (60000/64 = 938)\n",
    "\n",
    "Average loss for the epoch\n",
    "\n",
    "Store for plotting\n",
    "\n",
    "Print progress\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a516c",
   "metadata": {},
   "source": [
    "### Step 2.6 — Plot Loss\n",
    "\n",
    "What to look for:\n",
    "\n",
    "Decreasing: Model learning\n",
    "\n",
    "Plateau: Might need more epochs or different learning rate\n",
    "\n",
    "Increasing: Something wrong (learning rate too high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ec96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)           # Plot the training loss curve using the losses list, which contains the average loss for each epoch; This will help visualize how the loss changes over epochs, allowing us to see if the model is converging (loss decreasing) or if there are issues like overfitting (loss decreasing and then increasing) where x-axis represents the epoch number and y-axis represents the average loss for that epoch\n",
    "plt.xlabel(\"Epoch\")        # Label the x-axis as \"Epoch\"\n",
    "plt.ylabel(\"Loss\")         # Label the y-axis as \"Loss\"\n",
    "plt.title(\"Training Loss\") \n",
    "plt.show()                 # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50defd",
   "metadata": {},
   "source": [
    "### Step 2.7 — Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c9c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.6 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0                                               # Initialize a variable to count the number of correct predictions made by the model on the test dataset; This will be used to calculate the accuracy of the model after evaluating it on the test data\n",
    "total = 0                                                 # Initialize a variable to count the total number of predictions made by the model on the test dataset; This will be used as the denominator when calculating accuracy, which is the ratio of correct predictions to total predictions \n",
    "\n",
    "with torch.no_grad():                                     # Use torch.no_grad() to disable gradient computation during evaluation; This is important because we don't need gradients for evaluation, and it can save memory and computation time\n",
    "    for images, labels in test_loader:                    # Loop over the batches in the test dataset\n",
    "        outputs = model(images)                           # Forward pass: Pass the input images through the model to get the predicted outputs (logits) for each class\n",
    "        _, predicted = torch.max(outputs, 1)              # Get the predicted class by taking the index of the maximum logit for each image in the batch; torch.max(outputs, 1) returns both the maximum values and their indices along dimension 1 (the class dimension), and we use _ to ignore the max values and keep only the predicted class indices in predicted\n",
    "        total += labels.size(0)                           # Increment the total count by the number of labels in the current batch (labels.size(0) gives the batch size, which is the number of images in that batch); This keeps track of how many predictions we have made in total across all batches in the test dataset\n",
    "        correct += (predicted == labels).sum().item()     # Increment the correct count by the number of correct predictions in the current batch; (predicted == labels) creates a boolean tensor where True indicates a correct prediction, and .sum() counts the number of True values (correct predictions) in that batch; .item() converts the resulting tensor to a Python scalar and adds it to the correct count\n",
    "\n",
    "print(\"Accuracy:\", 100 * correct / total, \"%\")            # Calculate and print the accuracy of the model on the test dataset; Accuracy is computed as (correct / total) * 100 to get a percentage, which indicates how well the model performs on unseen data (the test set) after training; A higher accuracy means the model is better at correctly classifying the digits in the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f87b5",
   "metadata": {},
   "source": [
    "```\n",
    "python\n",
    "correct = 0\n",
    "total = 0\n",
    "Initialize counters for correct predictions and total samples.\n",
    "\n",
    "python\n",
    "with torch.no_grad():\n",
    "Why torch.no_grad()?\n",
    "\n",
    "Disables gradient computation\n",
    "\n",
    "Saves memory and computation\n",
    "\n",
    "We don't need gradients for testing\n",
    "\n",
    "Without this:\n",
    "\n",
    "PyTorch would build computation graph for every test image\n",
    "\n",
    "Wastes memory and time\n",
    "\n",
    "python\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "Breaking down the loop:\n",
    "1. Get batch:\n",
    "\n",
    "images.shape: (64, 1, 28, 28)\n",
    "\n",
    "labels.shape: (64,)\n",
    "\n",
    "2. Forward pass:\n",
    "\n",
    "outputs.shape: (64, 10) → logits for each class\n",
    "\n",
    "3. torch.max(outputs, 1):\n",
    "\n",
    "\n",
    "outputs = [[2.1, 0.5, -1.2, 3.4, ...],  # Image 1\n",
    "           [0.1, 4.2, -0.8, 1.2, ...],  # Image 2\n",
    "           ...]\n",
    "\n",
    "torch.max returns:\n",
    "values = [3.4, 4.2, ...]  # Maximum values\n",
    "indices = [3, 1, ...]      # Positions of maxima (the predicted digits)\n",
    "We only need indices (predicted digits), so _, predicted.\n",
    "\n",
    "4. total += labels.size(0):\n",
    "\n",
    "labels.size(0) = batch size (64)\n",
    "\n",
    "Keep running total of images processed\n",
    "\n",
    "5. (predicted == labels):\n",
    "\n",
    "\n",
    "predicted = [3, 1, 7, 0, 4, ...]\n",
    "labels    = [3, 1, 9, 0, 4, ...]\n",
    "Comparison = [True, True, False, True, True, ...]\n",
    "6. .sum().item():\n",
    "\n",
    ".sum() counts True values → tensor\n",
    "\n",
    ".item() converts to Python integer\n",
    "\n",
    "python\n",
    "print(\"Accuracy:\", 100 * correct / total, \"%\")\n",
    "Example:\n",
    "\n",
    "\n",
    "correct = 9,750\n",
    "total = 10,000\n",
    "accuracy = 100 × 9750/10000 = 97.5%\n",
    "Typical MNIST accuracy: 97-98% with this simple MLP\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
